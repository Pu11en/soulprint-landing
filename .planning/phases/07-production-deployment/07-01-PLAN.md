---
phase: 07-production-deployment
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - /home/drewpullen/clawd/soulprint-rlm/prompt_helpers.py
  - /home/drewpullen/clawd/soulprint-rlm/main.py
  - /home/drewpullen/clawd/soulprint-rlm/Dockerfile
  - /home/drewpullen/clawd/soulprint-rlm/tests/test_prompt_helpers.py
autonomous: true

must_haves:
  truths:
    - "Production RLM repo contains prompt_helpers.py with clean_section and format_section"
    - "Production main.py has build_rlm_system_prompt function using prompt_helpers"
    - "Production QueryRequest model accepts ai_name, sections, and web_search_context fields"
    - "Production /query endpoint uses build_rlm_system_prompt for all intent modes"
    - "Dockerfile COPYs prompt_helpers.py and verifies import at build time"
    - "All existing tests still pass after changes"
  artifacts:
    - path: "/home/drewpullen/clawd/soulprint-rlm/prompt_helpers.py"
      provides: "clean_section and format_section helpers"
      contains: "def clean_section"
    - path: "/home/drewpullen/clawd/soulprint-rlm/main.py"
      provides: "build_rlm_system_prompt function and updated /query endpoint"
      contains: "from prompt_helpers import clean_section, format_section"
    - path: "/home/drewpullen/clawd/soulprint-rlm/Dockerfile"
      provides: "Updated build with prompt_helpers.py copy and import verification"
      contains: "COPY prompt_helpers.py"
    - path: "/home/drewpullen/clawd/soulprint-rlm/tests/test_prompt_helpers.py"
      provides: "Unit tests for prompt_helpers in production repo"
      contains: "def test_clean_section"
  key_links:
    - from: "/home/drewpullen/clawd/soulprint-rlm/main.py"
      to: "prompt_helpers.py"
      via: "import statement"
      pattern: "from prompt_helpers import clean_section, format_section"
    - from: "/home/drewpullen/clawd/soulprint-rlm/main.py"
      to: "build_rlm_system_prompt"
      via: "function call in /query handler"
      pattern: "build_rlm_system_prompt"
---

<objective>
Copy Phase 6 prompt foundation code to the production soulprint-rlm repo and integrate it with the existing /query endpoint.

Purpose: The production RLM service currently builds hardcoded "You are SoulPrint" system prompts in the /query endpoint. Phase 6 created a proper prompt builder that uses structured personality sections, anti-generic rules, and memory context instructions. This plan copies that code to the production repo and wires it into all three intent modes (memory, realtime, normal).

Output: Production repo ready to deploy — prompt_helpers.py copied, main.py updated with build_rlm_system_prompt and new QueryRequest fields, Dockerfile updated, all tests passing.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-prompt-foundation/06-03-SUMMARY.md
@.planning/phases/07-production-deployment/07-RESEARCH.md

Key source files:
@/home/drewpullen/clawd/soulprint-landing/rlm-service/prompt_helpers.py
@/home/drewpullen/clawd/soulprint-landing/rlm-service/main.py (lines 195-285 for build_rlm_system_prompt)

Production target files:
@/home/drewpullen/clawd/soulprint-rlm/main.py (3751 lines — large file, handle carefully)
@/home/drewpullen/clawd/soulprint-rlm/Dockerfile
@/home/drewpullen/clawd/soulprint-rlm/tests/test_endpoints.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Copy prompt_helpers.py and update production main.py</name>
  <files>
    /home/drewpullen/clawd/soulprint-rlm/prompt_helpers.py
    /home/drewpullen/clawd/soulprint-rlm/main.py
    /home/drewpullen/clawd/soulprint-rlm/Dockerfile
    /home/drewpullen/clawd/soulprint-rlm/tests/test_prompt_helpers.py
  </files>
  <action>
    **Step 1: Copy prompt_helpers.py**

    Copy `/home/drewpullen/clawd/soulprint-landing/rlm-service/prompt_helpers.py` to `/home/drewpullen/clawd/soulprint-rlm/prompt_helpers.py`. This is a direct file copy — the module has zero external dependencies.

    **Step 2: Add import to production main.py**

    At the top of `/home/drewpullen/clawd/soulprint-rlm/main.py`, add:
    ```python
    from prompt_helpers import clean_section, format_section
    ```
    Place it after the existing imports (after the `load_dotenv()` line, near the other local imports).

    **Step 3: Add build_rlm_system_prompt function to production main.py**

    Copy the `build_rlm_system_prompt` function from `/home/drewpullen/clawd/soulprint-landing/rlm-service/main.py` (lines 195-285) into the production main.py. Place it BEFORE the existing `/query` endpoint handler (around line 2140, before `@app.post("/query")`). The function signature is:

    ```python
    def build_rlm_system_prompt(
        ai_name: str,
        sections: Optional[dict],
        soulprint_text: Optional[str],
        conversation_context: str,
        web_search_context: Optional[str] = None,
    ) -> str:
    ```

    This function uses `clean_section` and `format_section` from prompt_helpers, which was just imported.

    **Step 4: Update QueryRequest model**

    The production `QueryRequest` model (around line 301) is currently:
    ```python
    class QueryRequest(BaseModel):
        user_id: str
        message: str
        soulprint_text: Optional[str] = None
        history: Optional[List[dict]] = []
    ```

    Add the three missing fields that Next.js already sends:
    ```python
    class QueryRequest(BaseModel):
        user_id: str
        message: str
        soulprint_text: Optional[str] = None
        history: Optional[List[dict]] = []
        ai_name: Optional[str] = None
        sections: Optional[dict] = None
        web_search_context: Optional[str] = None
    ```

    **Step 5: Update /query endpoint to use build_rlm_system_prompt**

    The existing `/query` handler (line ~2145) has three intent modes: memory, realtime, normal. Each builds a hardcoded `system_prompt` string starting with "You are SoulPrint". Update ALL THREE modes to use `build_rlm_system_prompt` instead.

    The key change: Instead of building system_prompt inline, call:
    ```python
    ai_name = request.ai_name or "SoulPrint"
    ```
    at the top of the try block (replacing the hardcoded "SoulPrint" name), and for each intent mode, use `build_rlm_system_prompt()` to build the base prompt, then APPEND the intent-specific instructions after it.

    For the **memory** intent mode (line ~2169):
    ```python
    base_prompt = build_rlm_system_prompt(
        ai_name=ai_name,
        sections=request.sections,
        soulprint_text=soulprint,
        conversation_context="",  # Memory context handled separately below
        web_search_context=None,
    )
    # Then append the memory-specific CONVERSATION HISTORY and YOUR TASK sections
    system_prompt = base_prompt + f"""

## CONVERSATION HISTORY
The following are REAL conversations from the user's ChatGPT history. Use ONLY this data:

{limited_context}

## YOUR TASK
{intro}

Look through the conversation history above and tell them what you found. You MUST:
- Quote their EXACT words from the history above (copy-paste, don't paraphrase)
- Include the actual dates/timestamps shown in the history
- If multiple conversations mention the topic, summarize the pattern

IMPORTANT:
- ONLY use information from the conversation history above
- If the history above doesn't contain relevant conversations, say "I didn't find any conversations about this in your history"
- DO NOT make up quotes or dates
- DO NOT output template placeholders like [date] or [topic]"""
    ```

    For the **realtime** intent mode (line ~2201):
    ```python
    web_context = request.web_search_context or ""
    base_prompt = build_rlm_system_prompt(
        ai_name=ai_name,
        sections=request.sections,
        soulprint_text=soulprint,
        conversation_context="",
        web_search_context=web_context if web_context else None,
    )
    system_prompt = base_prompt + """

## REALTIME MODE
The user is asking about current/live information.
- If real-time data is provided above, use it and cite sources
- If no real-time data, answer but mention your knowledge cutoff
- Keep response focused on their specific question"""
    ```

    For the **normal** intent mode (line ~2231):
    ```python
    base_prompt = build_rlm_system_prompt(
        ai_name=ai_name,
        sections=request.sections,
        soulprint_text=soulprint,
        conversation_context="",
        web_search_context=None,
    )
    system_prompt = base_prompt + """

## NORMAL MODE - Direct Answer
The user is asking a general question. Your job:
1. **Answer their question directly** - Give a clear, helpful response
2. **Be knowledgeable** - Use your training to provide accurate info
3. **Be personal** - Use their profile to tailor the response style
4. **Keep it concise** - Don't over-explain unless asked

Answer the question naturally without forcing references to past conversations."""
    ```

    IMPORTANT: Preserve ALL existing logic around intent detection, memory search, chunk retrieval, memory_offer, the `bedrock_claude_message` call, and response formatting. ONLY replace the system_prompt construction within each intent branch.

    **Step 6: Update Dockerfile**

    In `/home/drewpullen/clawd/soulprint-rlm/Dockerfile`, add a COPY line for prompt_helpers.py. Place it after the `COPY main.py .` line:
    ```dockerfile
    COPY prompt_helpers.py .
    ```

    Also update the build-time import verification RUN command to include prompt_helpers:
    ```dockerfile
    RUN python -c "from prompt_helpers import clean_section, format_section; print('Prompt helpers OK')" && \
        python -c "from adapters import download_conversations, update_user_profile, save_chunks_batch; print('Adapters OK')" && \
        python -c "from processors.conversation_chunker import chunk_conversations; from processors.fact_extractor import extract_facts_parallel; from processors.memory_generator import generate_memory_section; from processors.v2_regenerator import regenerate_sections_v2; from processors.full_pass import run_full_pass_pipeline; print('Processors OK')"
    ```

    **Step 7: Add prompt_helpers unit tests**

    Create `/home/drewpullen/clawd/soulprint-rlm/tests/test_prompt_helpers.py` with basic tests for clean_section and format_section:
    - Test clean_section removes "not enough data" strings
    - Test clean_section removes "not enough data" from arrays
    - Test clean_section returns None for empty/null input
    - Test format_section produces markdown with sorted keys
    - Test format_section returns empty string for None input
    - Test format_section filters placeholders defensively

    These tests verify the module works correctly in the production repo context.

    **Step 8: Verify everything works**

    Run from the production repo root:
    ```bash
    cd /home/drewpullen/clawd/soulprint-rlm
    python3 -c "from prompt_helpers import clean_section, format_section; print('Imports OK')"
    python3 -m py_compile main.py
    python3 -m pytest tests/test_prompt_helpers.py -v
    python3 -m pytest tests/ -v
    ```

    All imports must succeed. All tests must pass. If existing tests fail due to the QueryRequest model change (new optional fields), update the test fixtures to account for the new fields (they're all Optional with defaults, so existing tests SHOULD pass without changes).
  </action>
  <verify>
    Run from production repo:
    ```bash
    cd /home/drewpullen/clawd/soulprint-rlm
    python3 -c "from prompt_helpers import clean_section, format_section; print('OK')"
    python3 -m py_compile main.py && echo "Syntax OK"
    python3 -m pytest tests/ -v
    ```
    All commands succeed with no errors.
  </verify>
  <done>
    - prompt_helpers.py exists in production repo root with clean_section and format_section
    - main.py imports from prompt_helpers and contains build_rlm_system_prompt function
    - QueryRequest has ai_name, sections, web_search_context fields
    - /query endpoint uses build_rlm_system_prompt in all three intent modes
    - Dockerfile COPYs prompt_helpers.py and verifies import at build time
    - All pytest tests pass (existing + new prompt_helpers tests)
  </done>
</task>

</tasks>

<verification>
1. `python3 -c "from prompt_helpers import clean_section, format_section; print('OK')"` succeeds in production repo
2. `python3 -m py_compile main.py` succeeds (no syntax errors)
3. `grep "from prompt_helpers import" main.py` finds the import
4. `grep "def build_rlm_system_prompt" main.py` finds the function
5. `grep "ai_name.*Optional" main.py` confirms QueryRequest updated
6. `grep "COPY prompt_helpers.py" Dockerfile` confirms Dockerfile updated
7. `pytest tests/ -v` all tests pass
</verification>

<success_criteria>
Production soulprint-rlm repo is ready to deploy: prompt_helpers.py present, main.py updated with Phase 6 prompt builder integrated into all /query intent modes, Dockerfile builds correctly, all tests pass. No git push yet — that's Plan 02.
</success_criteria>

<output>
After completion, create `.planning/phases/07-production-deployment/07-01-SUMMARY.md`
</output>
