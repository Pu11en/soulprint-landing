---
phase: 02-vector-infrastructure
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/20260211_hnsw_index_768.sql
  - rlm-service/processors/embedding_generator.py
  - rlm-service/processors/full_pass.py
  - rlm-service/requirements.txt
autonomous: false
user_setup:
  - service: supabase
    why: "HNSW index migration must be run manually in Supabase SQL Editor"
    dashboard_config:
      - task: "Run SQL migration in SQL Editor"
        location: "Supabase Dashboard -> SQL Editor -> New Query -> paste migration -> Run"

must_haves:
  truths:
    - "conversation_chunks.embedding column is vector(768) with HNSW index (not IVFFlat)"
    - "Full pass pipeline generates Titan Embed v2 (768-dim) embeddings for every saved chunk"
    - "Embeddings are stored in conversation_chunks.embedding column via Supabase REST PATCH"
    - "Embedding generation handles errors gracefully (logs warning, continues pipeline)"
    - "RPC functions match_conversation_chunks and match_conversation_chunks_by_tier accept vector(768)"
  artifacts:
    - path: "supabase/migrations/20260211_hnsw_index_768.sql"
      provides: "HNSW index migration with 768-dim column resize"
      contains: "USING hnsw"
    - path: "rlm-service/processors/embedding_generator.py"
      provides: "Batch Titan Embed v2 embedding generation via boto3"
      contains: "amazon.titan-embed-text-v2"
    - path: "rlm-service/processors/full_pass.py"
      provides: "Embedding generation step in full pass pipeline"
      contains: "generate_embeddings_batch"
  key_links:
    - from: "rlm-service/processors/full_pass.py"
      to: "rlm-service/processors/embedding_generator.py"
      via: "import generate_embeddings_batch"
      pattern: "from.*embedding_generator.*import"
    - from: "rlm-service/processors/embedding_generator.py"
      to: "Supabase REST API"
      via: "PATCH conversation_chunks with embedding vector"
      pattern: "rest/v1/conversation_chunks"
    - from: "rlm-service/processors/embedding_generator.py"
      to: "AWS Bedrock Titan Embed v2"
      via: "boto3 bedrock-runtime invoke_model"
      pattern: "invoke_model.*titan-embed"
---

<objective>
Add HNSW vector index and generate Titan Embed v2 embeddings for all conversation chunks during full pass.

Purpose: Enable semantic search over user conversation history. Currently chunks are saved to the database without embeddings, so vector similarity search returns nothing. After this plan, every chunk from the full pass pipeline will have a 768-dimensional embedding, stored with an HNSW index for fast approximate nearest neighbor queries.

Output:
- SQL migration replacing IVFFlat with HNSW index, resizing embedding column to vector(768)
- Python embedding generator module using AWS Bedrock Titan Embed v2
- Full pass pipeline integration that generates embeddings after chunk saves
</objective>

<execution_context>
@/home/drewpullen/.claude/get-shit-done/workflows/execute-plan.md
@/home/drewpullen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-pipeline-reliability/01-01-SUMMARY.md

Key reference files:
@rlm-service/processors/full_pass.py — Current full pass pipeline (chunks saved here, embeddings to be added after save)
@rlm-service/processors/conversation_chunker.py — Produces chunks that need embeddings
@rlm-service/requirements.txt — boto3 available via anthropic[bedrock] dependency
@rlm-service/processors/quick_pass.py — AWS Bedrock credential pattern (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION)
@lib/bedrock.ts — TypeScript Titan Embed v2 reference implementation (bedrockEmbed function)
@supabase/migrations/20260201_vector_1024.sql — Current vector(1024) column and RPC functions
@supabase/migrations/20260131_cleanup_database.sql — Current IVFFlat index definition
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create HNSW migration and Titan Embed v2 embedding generator</name>
  <files>
    supabase/migrations/20260211_hnsw_index_768.sql
    rlm-service/processors/embedding_generator.py
  </files>
  <action>
**1. SQL Migration (supabase/migrations/20260211_hnsw_index_768.sql):**

Write a SQL migration that:
- Clears existing embeddings: `UPDATE conversation_chunks SET embedding = NULL;` (they were generated with wrong model/dimensions or not at all)
- Alters embedding column: `ALTER TABLE conversation_chunks ALTER COLUMN embedding TYPE vector(768);`
- Drops existing IVFFlat index: `DROP INDEX IF EXISTS idx_conversation_chunks_embedding;`
- Creates HNSW index: `CREATE INDEX idx_conversation_chunks_embedding_hnsw ON conversation_chunks USING hnsw (embedding vector_cosine_ops) WITH (m = 16, ef_construction = 64);`
  - m=16, ef_construction=64 are good defaults for datasets under 1M rows (we have ~2000-5000 chunks per user, maybe 50 users = ~250K rows max)
- Updates `match_conversation_chunks` RPC function signature to accept `vector(768)` instead of `vector(1024)`
- Updates `match_conversation_chunks_by_tier` RPC function signature to accept `vector(768)` instead of `vector(1024)`
- Add comment on column: `COMMENT ON COLUMN conversation_chunks.embedding IS 'Titan Embed v2 (768 dimensions, cosine similarity, HNSW index)';`

**2. Embedding Generator (rlm-service/processors/embedding_generator.py):**

Create a new module that generates embeddings via AWS Bedrock Titan Embed v2:

```python
"""
Embedding Generator
Generates Titan Embed v2 embeddings (768 dims) for conversation chunks via AWS Bedrock.
"""
import os
import json
import boto3
import httpx
from typing import List, Dict, Optional

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_KEY = os.getenv("SUPABASE_SERVICE_KEY")

# Lazy-init Bedrock client
_bedrock_client = None

def get_bedrock_client():
    global _bedrock_client
    if _bedrock_client is None:
        _bedrock_client = boto3.client(
            'bedrock-runtime',
            region_name=os.environ.get('AWS_REGION', 'us-east-1'),
            aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),
        )
    return _bedrock_client


def embed_text(text: str, dimensions: int = 768) -> List[float]:
    """Generate a single embedding using Titan Embed v2.

    Args:
        text: Input text (will be truncated to 8000 chars for safety)
        dimensions: Output dimensions (768 default, Titan v2 supports 256-1024)

    Returns:
        List of floats representing the embedding vector
    """
    client = get_bedrock_client()
    truncated = text[:8000]  # Titan v2 max input is ~8192 tokens

    response = client.invoke_model(
        modelId='amazon.titan-embed-text-v2:0',
        contentType='application/json',
        accept='application/json',
        body=json.dumps({
            'inputText': truncated,
            'dimensions': dimensions,
            'normalize': True,
        }),
    )

    result = json.loads(response['body'].read())
    return result['embedding']


def embed_batch(texts: List[str], dimensions: int = 768) -> List[List[float]]:
    """Generate embeddings for a batch of texts.

    Titan Embed v2 does NOT support native batching — each text
    is a separate API call. We process sequentially to avoid
    rate limits (consistent with the concurrency=5 decision from Phase 1).

    Args:
        texts: List of input texts
        dimensions: Output dimensions (768 default)

    Returns:
        List of embedding vectors (same order as input)
    """
    embeddings = []
    for text in texts:
        embedding = embed_text(text, dimensions)
        embeddings.append(embedding)
    return embeddings
```

Add a function to update chunks in the database with their embeddings:

```python
async def update_chunk_embedding(chunk_id: str, embedding: List[float]):
    """Update a single conversation_chunks row with its embedding vector.

    Uses Supabase REST API PATCH to set the embedding column.
    The embedding is sent as a JSON array which PostgREST converts to vector.
    """
    async with httpx.AsyncClient() as client:
        response = await client.patch(
            f"{SUPABASE_URL}/rest/v1/conversation_chunks?id=eq.{chunk_id}",
            json={"embedding": embedding},
            headers={
                "apikey": SUPABASE_SERVICE_KEY,
                "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                "Content-Type": "application/json",
                "Prefer": "return=minimal",
            },
            timeout=30.0,
        )
        if response.status_code not in (200, 204):
            raise RuntimeError(f"Failed to update embedding for chunk {chunk_id}: {response.status_code}")
```

Add the main orchestration function:

```python
async def generate_embeddings_for_chunks(user_id: str, batch_size: int = 50):
    """Generate embeddings for all conversation_chunks that don't have one yet.

    Fetches chunks without embeddings, generates Titan Embed v2 embeddings,
    and PATCHes them back to the database. Processes in batches to limit memory.

    Args:
        user_id: User ID whose chunks need embeddings
        batch_size: Number of chunks to process per batch (default 50)

    Returns:
        Number of chunks embedded
    """
    total_embedded = 0
    offset = 0

    while True:
        # Fetch chunks without embeddings
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{SUPABASE_URL}/rest/v1/conversation_chunks",
                params={
                    "user_id": f"eq.{user_id}",
                    "embedding": "is.null",
                    "select": "id,content",
                    "limit": str(batch_size),
                    "offset": str(offset),
                },
                headers={
                    "apikey": SUPABASE_SERVICE_KEY,
                    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                },
                timeout=30.0,
            )
            if response.status_code != 200:
                print(f"[Embeddings] Failed to fetch chunks: {response.status_code}")
                break

            chunks = response.json()

        if not chunks:
            break  # No more chunks to process

        print(f"[Embeddings] Processing batch of {len(chunks)} chunks (offset={offset})")

        # Generate embeddings for this batch
        texts = [chunk["content"] for chunk in chunks]
        embeddings = embed_batch(texts)  # Sequential Titan v2 calls

        # Update each chunk with its embedding
        for chunk, embedding in zip(chunks, embeddings):
            await update_chunk_embedding(chunk["id"], embedding)

        total_embedded += len(chunks)
        offset += batch_size

    print(f"[Embeddings] Generated {total_embedded} embeddings for user {user_id}")
    return total_embedded
```

Important implementation notes:
- `embed_text` and `embed_batch` are synchronous (boto3 is sync) — this is fine since they're called from async context and Bedrock calls are fast (~50ms each)
- No need to add boto3 to requirements.txt — it's already installed via `anthropic[bedrock]` dependency
- Use the SAME AWS credentials pattern as quick_pass.py (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION from env)
- 768 dimensions, not 1024 — aligns with ROADMAP decision and reduces storage/search cost
- Truncate input to 8000 chars (Titan v2 max is ~8192 tokens but chars are safer)
- normalize=True for cosine similarity compatibility
  </action>
  <verify>
1. File exists: `ls rlm-service/processors/embedding_generator.py`
2. Migration file exists: `ls supabase/migrations/20260211_hnsw_index_768.sql`
3. Migration contains HNSW: `grep -c "hnsw" supabase/migrations/20260211_hnsw_index_768.sql` returns > 0
4. Migration contains vector(768): `grep -c "vector(768)" supabase/migrations/20260211_hnsw_index_768.sql` returns > 0
5. Embedding generator imports boto3: `grep -c "boto3" rlm-service/processors/embedding_generator.py` returns > 0
6. Embedding generator uses Titan v2: `grep -c "titan-embed-text-v2" rlm-service/processors/embedding_generator.py` returns > 0
7. Python syntax valid: `python3 -c "import ast; ast.parse(open('rlm-service/processors/embedding_generator.py').read())"` succeeds
  </verify>
  <done>
- SQL migration ready to run in Supabase SQL Editor (HNSW index, vector(768), updated RPC functions)
- embedding_generator.py module with embed_text(), embed_batch(), generate_embeddings_for_chunks()
- Uses boto3 with AWS credentials from environment (same as quick_pass.py)
- 768-dim Titan Embed v2 embeddings with normalize=True
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire embedding generation into full pass pipeline</name>
  <files>
    rlm-service/processors/full_pass.py
  </files>
  <action>
Modify `run_full_pass_pipeline()` in `rlm-service/processors/full_pass.py` to generate embeddings after chunks are saved to the database.

**Add embedding step between Step 3 (save chunks) and Step 4 (extract facts):**

After the chunk save loop (around line 180, after `print(f"[FullPass] Saved {len(chunks)} chunks to database")`), add:

```python
# Step 3.5: Generate embeddings for saved chunks
try:
    from processors.embedding_generator import generate_embeddings_for_chunks
    embedded_count = await generate_embeddings_for_chunks(user_id)
    print(f"[FullPass] Generated embeddings for {embedded_count} chunks")
except Exception as e:
    # Non-fatal: embeddings can be regenerated later, don't fail the pipeline
    print(f"[FullPass] WARNING: Embedding generation failed: {e}")
    # Pipeline continues — chunks are saved, facts can still be extracted
```

Key design decision: Embedding generation is **non-fatal**. If it fails:
- Chunks are already saved (from Step 3)
- Fact extraction continues (Step 4)
- Memory generation continues (Step 7)
- Embeddings can be backfilled later via retry or a separate script
- This matches the "fail-soft" pattern from Phase 1 (errors are logged, not swallowed silently)

The embedding step goes AFTER chunk saves (needs chunk IDs in database) and BEFORE fact extraction (so facts and embeddings can run independently in future). Placing it here means the database write (chunks) is complete before we start the relatively slow embedding generation (~50ms per chunk, ~100 chunks = ~5s).

Do NOT change the function signature or return value. The only change is inserting the embedding step.
  </action>
  <verify>
1. `grep -c "generate_embeddings_for_chunks" rlm-service/processors/full_pass.py` returns > 0
2. `grep -c "Embedding generation failed" rlm-service/processors/full_pass.py` returns > 0 (non-fatal handling)
3. `grep -c "embedding_generator" rlm-service/processors/full_pass.py` returns > 0 (import)
4. `python3 -c "import ast; ast.parse(open('rlm-service/processors/full_pass.py').read())"` succeeds
5. `npm run build` succeeds (no TypeScript impact, but validates overall project)
  </verify>
  <done>
- Full pass pipeline generates embeddings for all chunks after saving them
- Embedding failure is non-fatal (logged as warning, pipeline continues)
- Embeddings stored in conversation_chunks.embedding column
- Pipeline ordering: save chunks -> generate embeddings -> extract facts -> generate memory
  </done>
</task>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 3: Run HNSW migration in Supabase SQL Editor</name>
  <action>
The SQL migration file has been created at `supabase/migrations/20260211_hnsw_index_768.sql`.

Run this migration in Supabase SQL Editor to:
1. Clear existing (stale) embeddings
2. Resize embedding column from vector(1024) to vector(768)
3. Replace IVFFlat index with HNSW index
4. Update RPC function signatures to vector(768)
  </action>
  <how-to-verify>
1. Go to Supabase Dashboard -> SQL Editor -> New Query
2. Copy/paste contents of `supabase/migrations/20260211_hnsw_index_768.sql`
3. Click "Run"
4. Verify: Run `SELECT column_name, data_type, udt_name FROM information_schema.columns WHERE table_name = 'conversation_chunks' AND column_name = 'embedding';` -- should show the column exists
5. Verify: Run `SELECT indexname, indexdef FROM pg_indexes WHERE tablename = 'conversation_chunks' AND indexname LIKE '%hnsw%';` -- should show HNSW index
  </how-to-verify>
  <resume-signal>Type "done" after running the migration successfully, or describe any errors</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:
1. `rlm-service/processors/embedding_generator.py` exists with Titan Embed v2 via boto3
2. `rlm-service/processors/full_pass.py` calls `generate_embeddings_for_chunks` after chunk saves
3. `supabase/migrations/20260211_hnsw_index_768.sql` exists with HNSW + vector(768)
4. Migration has been run in Supabase (HNSW index active, vector(768) column)
5. Python files parse without syntax errors
6. Project builds successfully (`npm run build`)
</verification>

<success_criteria>
- HNSW index active on conversation_chunks.embedding in Supabase
- Titan Embed v2 (768-dim) embedding generator module created and tested
- Full pass pipeline generates embeddings after saving chunks
- Embedding failures are non-fatal (pipeline continues)
- Next full pass import will produce chunks WITH embeddings
</success_criteria>

<output>
After completion, create `.planning/phases/02-vector-infrastructure/02-01-SUMMARY.md`
</output>
