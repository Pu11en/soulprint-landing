---
phase: 04-quality-scoring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/evaluation/quality-judges.ts
  - lib/evaluation/quality-scoring.ts
  - supabase/migrations/20260209_quality_breakdown.sql
autonomous: true

must_haves:
  truths:
    - "Each soulprint section (SOUL, IDENTITY, USER, AGENTS, TOOLS) can be scored 0-100 for completeness, coherence, and specificity"
    - "Quality scores are stored in user_profiles.quality_breakdown JSONB column"
    - "Profiles with any metric below 60 can be queried efficiently"
  artifacts:
    - path: "lib/evaluation/quality-judges.ts"
      provides: "CompletenessJudge, CoherenceJudge, SpecificityJudge extending BaseMetric"
      exports: ["CompletenessJudge", "CoherenceJudge", "SpecificityJudge"]
    - path: "lib/evaluation/quality-scoring.ts"
      provides: "Scoring orchestrator functions and type definitions"
      exports: ["scoreSoulprintSection", "calculateQualityBreakdown", "SectionQualityScores", "QualityBreakdown"]
    - path: "supabase/migrations/20260209_quality_breakdown.sql"
      provides: "Database migration for quality_breakdown JSONB column"
      contains: "quality_breakdown"
  key_links:
    - from: "lib/evaluation/quality-judges.ts"
      to: "lib/evaluation/judges.ts"
      via: "Same BaseMetric extension pattern, bedrockChatJSON, HAIKU_45"
      pattern: "extends BaseMetric"
    - from: "lib/evaluation/quality-scoring.ts"
      to: "lib/evaluation/quality-judges.ts"
      via: "Imports judge classes and runs them in parallel"
      pattern: "new CompletenessJudge|new CoherenceJudge|new SpecificityJudge"
---

<objective>
Create quality scoring infrastructure: three LLM-as-judge classes for soulprint section quality assessment, a scoring orchestrator, and the database migration for storing results.

Purpose: This is the foundation layer that everything else in Phase 4 depends on. Quality judges score soulprint sections on three dimensions (completeness, coherence, specificity) using the proven BaseMetric pattern from Phase 1. The JSONB column stores scores efficiently for both prompt injection and threshold-based refinement queries.

Output: Three judge classes, a scoring orchestrator with types, and a SQL migration ready for execution.
</objective>

<execution_context>
@./.claude/agents/gsd-planner.md
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-quality-scoring/04-RESEARCH.md
@lib/evaluation/judges.ts
@lib/evaluation/types.ts
@lib/soulprint/types.ts
@lib/bedrock.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Quality Judge Classes</name>
  <files>lib/evaluation/quality-judges.ts</files>
  <action>
Create `lib/evaluation/quality-judges.ts` with three judge classes extending Opik's BaseMetric, following the exact same pattern as `lib/evaluation/judges.ts` (PersonalityConsistencyJudge, FactualityJudge, ToneMatchingJudge).

Import from the same sources as existing judges:
```
import { BaseMetric, z } from 'opik';
import type { EvaluationScoreResult } from 'opik';
import { bedrockChatJSON } from '@/lib/bedrock';
```

Create a shared Zod schema for all three judges:
```typescript
const qualitySchema = z.object({
  section_name: z.string(),     // 'SOUL', 'IDENTITY', 'USER', 'AGENTS', 'TOOLS'
  section_content: z.string(),
  section_type: z.enum(['soul', 'identity', 'user', 'agents', 'tools']),
});
```

Reuse the `clampScore`, `ANTI_LENGTH_BIAS`, `JUDGE_SYSTEM`, and `JudgeResponse` patterns from `judges.ts`. Duplicate them locally in this file (do NOT import from judges.ts -- keep quality judges self-contained like the existing evaluation judges are self-contained).

**CompletenessJudge:**
- Name: 'completeness'
- Evaluates: Are all expected fields present and populated with sufficient detail?
- Has per-section-type expected fields lookup:
  - soul: communication_style, personality_traits, tone_preferences, boundaries, humor_style, formality_level, emotional_patterns
  - identity: ai_name, archetype, vibe, emoji_style, signature_greeting
  - user: name, location, occupation, relationships, interests, life_context, preferred_address
  - agents: response_style, behavioral_rules, context_adaptation, memory_directives, do_not
  - tools: likely_usage, capabilities_emphasis, output_preferences, depth_preference
- Include expected fields in prompt to guide evaluation
- Score scale: 1.0 = all fields present with detail, 0.7-0.9 = mostly complete, 0.4-0.6 = several gaps, 0.0-0.3 = critical fields missing

**CoherenceJudge:**
- Name: 'coherence'
- Evaluates: Logical flow, internal consistency, structural clarity, unified purpose
- Does NOT need expected fields (coherence is about organization, not completeness)
- Score scale: 1.0 = perfectly logical and consistent, 0.0 = incoherent/contradictory

**SpecificityJudge:**
- Name: 'specificity'
- Evaluates: Are details concrete vs. vague/generic?
- Focus: Does content contain specific names, numbers, examples rather than generic platitudes?
- Score scale: 1.0 = highly specific, concrete details, 0.0 = entirely generic/vague

All judges:
- Use `bedrockChatJSON` with model `'HAIKU_45'`, maxTokens 512, temperature 0.1
- Include `ANTI_LENGTH_BIAS` in every prompt
- Use `JUDGE_SYSTEM` as system message
- Clamp output to [0.0, 1.0] with `clampScore`
- Return `scoringFailed: true` on validation or API errors
- Respond with JSON `{ score: number, reasoning: string }`

Export all three classes.
  </action>
  <verify>
Run `npx tsc --noEmit lib/evaluation/quality-judges.ts` -- no type errors. Verify the file imports resolve and all three classes export correctly.
  </verify>
  <done>
CompletenessJudge, CoherenceJudge, and SpecificityJudge classes exist in quality-judges.ts, extend BaseMetric, follow identical patterns to existing judges.ts, and compile without errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Scoring Orchestrator and Database Migration</name>
  <files>lib/evaluation/quality-scoring.ts, supabase/migrations/20260209_quality_breakdown.sql</files>
  <action>
**Part A: Create `lib/evaluation/quality-scoring.ts`**

Define and export types:
```typescript
export interface SectionQualityScores {
  completeness: number; // 0-100
  coherence: number;    // 0-100
  specificity: number;  // 0-100
}

export interface QualityBreakdown {
  soul: SectionQualityScores;
  identity: SectionQualityScores;
  user: SectionQualityScores;
  agents: SectionQualityScores;
  tools: SectionQualityScores;
}
```

Create `scoreSoulprintSection(sectionType, content)`:
- Takes section type ('soul' | 'identity' | 'user' | 'agents' | 'tools') and content string
- Instantiates all three judges
- Runs all three in parallel via `Promise.all`
- Normalizes 0.0-1.0 scores to 0-100 integers: `Math.round(Math.min(100, Math.max(0, value * 100)))`
- Returns `SectionQualityScores`

Create `calculateQualityBreakdown(profile)`:
- Takes profile object with `{ soul_md, identity_md, user_md, agents_md, tools_md }` (all `string | null`)
- Scores all 5 sections in parallel (each section scores 3 dimensions in parallel internally = 15 total LLM calls)
- For null/empty sections, return `{ completeness: 0, coherence: 0, specificity: 0 }`
- Returns full `QualityBreakdown`

Create `hasLowQualityScores(breakdown, threshold = 60)`:
- Checks if any metric in any section is below threshold
- Returns `boolean`

Create `getLowQualitySections(breakdown, threshold = 60)`:
- Returns array of `{ section, metric, score }` objects for all below-threshold items
- Useful for logging and refinement targeting

Export all functions and types.

**Part B: Create SQL migration `supabase/migrations/20260209_quality_breakdown.sql`**

```sql
-- Phase 4: Quality Scoring - Add quality breakdown storage
-- Stores per-section quality scores (completeness, coherence, specificity) as JSONB

ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS quality_breakdown JSONB;

COMMENT ON COLUMN public.user_profiles.quality_breakdown IS
  'Quality scores (0-100) per soulprint section: { soul: { completeness, coherence, specificity }, identity: {...}, ... }';

-- GIN index for efficient JSONB querying (threshold checks, section lookups)
CREATE INDEX IF NOT EXISTS idx_user_profiles_quality_breakdown
  ON public.user_profiles USING GIN (quality_breakdown);

-- Timestamp for when scores were last calculated
ALTER TABLE public.user_profiles ADD COLUMN IF NOT EXISTS quality_scored_at TIMESTAMPTZ;

-- Function to find profiles with any quality metric below threshold
CREATE OR REPLACE FUNCTION find_low_quality_profiles(threshold_score INT DEFAULT 60)
RETURNS TABLE(user_id UUID) AS $$
BEGIN
  RETURN QUERY
  SELECT up.user_id
  FROM user_profiles up
  WHERE up.quality_breakdown IS NOT NULL
    AND EXISTS (
      SELECT 1
      FROM jsonb_each(up.quality_breakdown) AS section(key, value)
      CROSS JOIN jsonb_each(section.value) AS metric(metric_key, metric_value)
      WHERE (metric_value::text)::int < threshold_score
    );
END;
$$ LANGUAGE plpgsql;
```
  </action>
  <verify>
Run `npx tsc --noEmit lib/evaluation/quality-scoring.ts` -- no type errors. Verify the SQL file exists and contains the ALTER TABLE, CREATE INDEX, and CREATE FUNCTION statements. Verify quality-scoring.ts imports from quality-judges.ts correctly.
  </verify>
  <done>
- `quality-scoring.ts` exports `scoreSoulprintSection`, `calculateQualityBreakdown`, `hasLowQualityScores`, `getLowQualitySections`, `SectionQualityScores`, and `QualityBreakdown`
- SQL migration adds `quality_breakdown` JSONB column, `quality_scored_at` timestamp, GIN index, and `find_low_quality_profiles()` function
- Types compile without errors
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors in the new files
2. `lib/evaluation/quality-judges.ts` exports three classes following BaseMetric pattern
3. `lib/evaluation/quality-scoring.ts` exports orchestrator functions and types
4. SQL migration file exists with correct schema changes
5. No modifications to existing files (judges.ts, types.ts remain untouched)
</verification>

<success_criteria>
- Three quality judge classes exist and extend BaseMetric with identical patterns to Phase 1 judges
- Scoring orchestrator can score all 5 sections x 3 dimensions in parallel
- Quality scores normalize from 0.0-1.0 to 0-100 integer range
- Database migration adds quality_breakdown JSONB + quality_scored_at + GIN index + threshold query function
- All code compiles without type errors
</success_criteria>

<output>
After completion, create `.planning/phases/04-quality-scoring/04-01-SUMMARY.md`
</output>
