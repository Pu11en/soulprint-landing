---
phase: 03-memory-in-chat
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/memory/query.ts
autonomous: true

must_haves:
  truths:
    - "Next.js Bedrock fallback path embeds queries with Titan Embed v2 (768 dimensions), not Cohere v3 (1024)"
    - "Vector search in getMemoryContext uses match_conversation_chunks RPC (cosine similarity), not timestamp sort"
    - "Bedrock fallback chat responses include semantically relevant memory chunks"
  artifacts:
    - path: "lib/memory/query.ts"
      provides: "Titan Embed v2 query embedding and simplified semantic search"
      contains: "titan-embed-text-v2"
  key_links:
    - from: "lib/memory/query.ts"
      to: "AWS Bedrock Titan Embed v2"
      via: "InvokeModelCommand with titan-embed-text-v2:0"
      pattern: "titan-embed-text-v2"
    - from: "lib/memory/query.ts"
      to: "Supabase RPC match_conversation_chunks"
      via: "supabase.rpc('match_conversation_chunks')"
      pattern: "match_conversation_chunks"
---

<objective>
Fix the Next.js memory query module to use Titan Embed v2 (768-dim) for query embeddings and simplify the search to use the proven match_conversation_chunks RPC function.

Purpose: The current `lib/memory/query.ts` has two critical bugs: (1) it uses Cohere Embed v3 which produces 1024-dim vectors, but the database now stores 768-dim Titan Embed v2 vectors — a dimension mismatch that would cause every similarity search to fail; (2) it calls `match_conversation_chunks_layered` which doesn't exist as an RPC function. This plan fixes both issues so the Bedrock fallback path (used when RLM is down) also gets semantic memory.

Output: `lib/memory/query.ts` uses Titan Embed v2 for query embedding and calls the existing `match_conversation_chunks` / `match_conversation_chunks_by_tier` RPC functions.
</objective>

<execution_context>
@/home/drewpullen/.claude/get-shit-done/workflows/execute-plan.md
@/home/drewpullen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-vector-infrastructure/02-01-SUMMARY.md

Key references:
@lib/memory/query.ts — Current memory query with Cohere v3 and broken layered search
@rlm-service/processors/embedding_generator.py — Reference for Titan Embed v2 API format
@supabase/migrations/20260211_hnsw_index_768.sql — Actual RPC function signatures (match_conversation_chunks, match_conversation_chunks_by_tier)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Switch embedQuery from Cohere v3 to Titan Embed v2 (768-dim)</name>
  <files>lib/memory/query.ts</files>
  <action>
Replace the `embedQuery()` function in `lib/memory/query.ts` to use Amazon Titan Embed v2 instead of Cohere Embed v3. This matches the embedding model used during the full pass pipeline (in `rlm-service/processors/embedding_generator.py`).

**Replace the existing `embedQuery` function** (around lines 92-123) with:

```typescript
/**
 * Embed a query using Amazon Titan Embed v2 (768 dimensions)
 * Must match the model used in rlm-service/processors/embedding_generator.py
 */
export async function embedQuery(text: string): Promise<number[]> {
  const client = getBedrockClient();

  const command = new InvokeModelCommand({
    modelId: 'amazon.titan-embed-text-v2:0',
    contentType: 'application/json',
    accept: 'application/json',
    body: JSON.stringify({
      inputText: text.slice(0, 8000), // Titan v2 max ~8192 tokens, truncate conservatively
      dimensions: 768,
      normalize: true,
    }),
  });

  const embedPromise = async (): Promise<number[]> => {
    const response = await client.send(command);
    const result = JSON.parse(new TextDecoder().decode(response.body));
    return result.embedding; // Titan v2 returns { embedding: number[] }
  };

  try {
    const result = await withTimeout(embedPromise(), EMBEDDING_TIMEOUT_MS, 'embedQuery');
    if (result === null) {
      throw new Error('Embedding timed out');
    }
    return result;
  } catch (error) {
    console.error('[Embed] Titan v2 error:', error);
    throw new Error(`Titan embedding error: ${error}`);
  }
}
```

Key differences from Cohere v3:
- Model ID: `amazon.titan-embed-text-v2:0` (not `cohere.embed-english-v3`)
- Input field: `inputText` (not `texts` array)
- Output: `result.embedding` (not `result.embeddings.float[0]`)
- No `input_type` or `embedding_types` params (Titan doesn't use these)
- Truncation at 8000 chars (matching the Python embed_text)
- Dimensions: 768 explicitly set
- Normalize: true (unit vectors for cosine similarity)

**Also update `embedBatch()`** (around lines 128-159) to use Titan v2:

```typescript
/**
 * Batch embed multiple texts using Titan Embed v2
 * Note: Titan v2 does NOT support native batching, so we call sequentially
 */
export async function embedBatch(texts: string[]): Promise<number[][]> {
  const allEmbeddings: number[][] = [];

  for (const text of texts) {
    const embedding = await embedQuery(text);
    allEmbeddings.push(embedding);
  }

  return allEmbeddings;
}
```

This simplifies embedBatch to just call embedQuery sequentially, matching the Python implementation. No need for the BATCH_SIZE = 96 logic since Titan v2 doesn't support batch requests.
  </action>
  <verify>
1. `npm run build` succeeds
2. Grep confirms `titan-embed-text-v2` in lib/memory/query.ts
3. Grep confirms `cohere.embed-english-v3` does NOT appear in lib/memory/query.ts
4. Grep confirms `dimensions: 768` in embedQuery
  </verify>
  <done>
embedQuery() uses Amazon Titan Embed v2 (768-dim) matching the storage model. embedBatch() simplified to sequential calls matching Python implementation.
  </done>
</task>

<task type="auto">
  <name>Task 2: Simplify getMemoryContext to use match_conversation_chunks RPC</name>
  <files>lib/memory/query.ts</files>
  <action>
Replace the broken layered search approach in `getMemoryContext()` with a simpler, working approach using the actual `match_conversation_chunks` RPC function.

**Problem:** The current code calls `searchMemoryLayered()` which uses `match_conversation_chunks_layered` RPC — this function does NOT exist in the database. It falls back to `match_conversation_chunks` but the `layer_index` parameter doesn't map to the actual `chunk_tier` column.

**Replace `searchMemoryLayered()`** (around lines 164-228) with a simpler function that uses the actual RPC:

```typescript
/**
 * Search conversation_chunks by vector similarity using match_conversation_chunks RPC
 */
export async function searchChunksSemantic(
  userId: string,
  queryEmbedding: number[],
  topK: number = 10,
  minSimilarity: number = 0.3,
): Promise<MemoryChunk[]> {
  const supabase = getSupabaseAdmin();

  const searchPromise = async (): Promise<MemoryChunk[]> => {
    const { data, error } = await supabase.rpc('match_conversation_chunks', {
      query_embedding: queryEmbedding,
      match_user_id: userId,
      match_count: topK,
      match_threshold: minSimilarity,
    });

    if (error) {
      console.error('[Memory] Semantic search RPC error:', error.message);
      throw new Error(`Failed to search memory: ${error.message}`);
    }

    return (data || []).map((row: ChunkRpcRow) => ({
      id: row.id,
      title: row.title || 'Untitled',
      content: row.content,
      created_at: row.created_at,
      similarity: row.similarity,
      layer_index: 1, // Not used in new approach, kept for interface compat
    }));
  };

  const result = await withTimeout(
    searchPromise(),
    MEMORY_SEARCH_TIMEOUT_MS,
    'searchChunksSemantic'
  );

  return result || [];
}
```

**Important:** Use `getSupabaseAdmin()` (not `await createClient()`) because the RPC functions need the service role key to bypass RLS and access the user's chunks.

**Replace the `getMemoryContext()` function** (around lines 321-439) with a simplified version:

```typescript
/**
 * Get memory context for a chat query via semantic search.
 * Embeds the query with Titan Embed v2, searches conversation_chunks via cosine
 * similarity, and formats results as context text for the LLM.
 *
 * Falls back to keyword search if embedding/vector search fails.
 */
export async function getMemoryContext(
  userId: string,
  query: string,
  maxChunks: number = 10
): Promise<{ chunks: MemoryChunk[]; contextText: string; method: string; learnedFacts: LearnedFactResult[] }> {
  const emptyResult = { chunks: [], contextText: '', method: 'timeout', learnedFacts: [] };

  const fetchContext = async () => {
    let chunks: MemoryChunk[] = [];
    let learnedFacts: LearnedFactResult[] = [];
    let method = 'none';

    try {
      // Embed query with Titan v2 (768-dim)
      const queryEmbedding = await embedQuery(query);

      // Semantic search via match_conversation_chunks RPC
      chunks = await searchChunksSemantic(userId, queryEmbedding, maxChunks, 0.3);
      method = chunks.length > 0 ? 'semantic_vector' : 'none';

      console.log(`[Memory] Semantic search found ${chunks.length} relevant chunks for user ${userId}`);

      // Also get learned facts
      learnedFacts = await searchLearnedFacts(userId, queryEmbedding, 10, 0.4);

    } catch (error) {
      console.log('[Memory] Vector search failed:', error);
      // Fallback to keyword search
      chunks = await keywordSearch(userId, query, maxChunks);
      method = chunks.length > 0 ? 'keyword' : 'none';
    }

    if (chunks.length === 0 && learnedFacts.length === 0) {
      return { chunks: [], contextText: '', method, learnedFacts: [] };
    }

    // Format context
    const contextParts: string[] = [];

    // Facts first
    if (learnedFacts.length > 0) {
      const factsText = learnedFacts
        .map(f => `- [Fact] ${f.fact}`)
        .join('\n');
      contextParts.push(`[Learned Facts]\n${factsText}`);
    }

    // Memory chunks (sorted by similarity, highest first)
    if (chunks.length > 0) {
      const formattedChunks = chunks.map(chunk => {
        const maxLen = 2000;
        const truncated = chunk.content.length > maxLen
          ? chunk.content.slice(0, maxLen) + '...'
          : chunk.content;
        return `[Memory: ${chunk.title}] (relevance: ${chunk.similarity.toFixed(2)})\n${truncated}`;
      }).join('\n\n');
      contextParts.push(formattedChunks);
    }

    return {
      chunks,
      contextText: contextParts.join('\n\n---\n\n'),
      method,
      learnedFacts
    };
  };

  // Wrap entire memory fetch with overall timeout
  const result = await withTimeout(fetchContext(), MEMORY_CONTEXT_TIMEOUT_MS, 'getMemoryContext');
  if (result === null) {
    console.warn('[Memory] Entire memory context fetch timed out, proceeding without memory');
    return emptyResult;
  }
  return result;
}
```

Key simplifications:
- No more layered search (Macro/Thematic/Micro) — just top-K across all chunks
- No more `layer_index` filtering — the `match_conversation_chunks` RPC already does HNSW search across all chunks
- Chunks sorted by similarity (the RPC returns them sorted)
- Uses `getSupabaseAdmin()` for service-role access
- Same timeout protection and fallback patterns

**Also remove the now-unused `searchMemoryLayered()` function** since it's replaced by `searchChunksSemantic()`. Keep `keywordSearch()` as fallback.

**Update the `ChunkRpcRow` interface** to match the actual RPC return columns:

```typescript
interface ChunkRpcRow {
  id: string;
  user_id: string;
  conversation_id: string;
  title: string | null;
  content: string;
  chunk_tier: string;
  message_count: number;
  created_at: string;
  similarity: number;
}
```

Remove the `layer_index` from the interface (the RPC returns `chunk_tier` text, not `layer_index` number). The `MemoryChunk` interface can keep `layer_index` for backward compatibility but it will always be 1.
  </action>
  <verify>
1. `npm run build` succeeds
2. Grep confirms `searchChunksSemantic` function exists in lib/memory/query.ts
3. Grep confirms `match_conversation_chunks_layered` does NOT appear in lib/memory/query.ts
4. Grep confirms `semantic_vector` method string in lib/memory/query.ts
5. Grep confirms `getSupabaseAdmin` is used for RPC calls (not `createClient`)
  </verify>
  <done>
getMemoryContext() embeds queries with Titan Embed v2, calls match_conversation_chunks RPC for cosine similarity search, and formats top-K results as context text. Broken layered search removed. Keyword search preserved as fallback.
  </done>
</task>

</tasks>

<verification>
1. lib/memory/query.ts uses `amazon.titan-embed-text-v2:0` (not `cohere.embed-english-v3`)
2. Query embeddings are 768 dimensions (matching stored embeddings)
3. `match_conversation_chunks` RPC used for search (not `match_conversation_chunks_layered`)
4. `getSupabaseAdmin()` used for RPC calls (service role key needed)
5. Keyword search fallback still works
6. `npm run build` succeeds
</verification>

<success_criteria>
- embedQuery() produces 768-dim Titan Embed v2 vectors matching the stored embeddings
- getMemoryContext() returns semantically relevant chunks via cosine similarity
- The broken layered search code is removed
- Bedrock fallback path gets memory context that actually works (vectors match)
- Build succeeds with no type errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-memory-in-chat/03-02-SUMMARY.md`
</output>
