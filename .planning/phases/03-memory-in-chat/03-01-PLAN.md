---
phase: 03-memory-in-chat
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - rlm-service/main.py
autonomous: true

must_haves:
  truths:
    - "RLM /query endpoint retrieves conversation chunks via cosine similarity search, not timestamp sort"
    - "Chat responses reference specific facts from user's history when relevant"
    - "memory_md from user_profiles is included in the system prompt sent to Claude"
  artifacts:
    - path: "rlm-service/main.py"
      provides: "Semantic search in /query endpoint"
      contains: "embed_text"
  key_links:
    - from: "rlm-service/main.py"
      to: "rlm-service/processors/embedding_generator.py"
      via: "import embed_text for query embedding"
      pattern: "from processors\\.embedding_generator import embed_text"
    - from: "rlm-service/main.py"
      to: "Supabase RPC match_conversation_chunks"
      via: "httpx POST to /rest/v1/rpc/match_conversation_chunks"
      pattern: "match_conversation_chunks"
---

<objective>
Replace the timestamp-sorted chunk retrieval in the RLM /query endpoint with semantic vector search using Titan Embed v2 embeddings.

Purpose: The RLM /query endpoint currently fetches the 100 most recent conversation chunks via `get_conversation_chunks()` (ORDER BY created_at DESC). This wastes context window on irrelevant chunks. Semantic search retrieves the 5-10 most relevant chunks for the user's actual message, dramatically improving response quality.

Output: RLM /query endpoint uses embed_text() to embed the user's query, calls match_conversation_chunks RPC for cosine similarity search, and passes relevant chunks as context to Claude.
</objective>

<execution_context>
@/home/drewpullen/.claude/get-shit-done/workflows/execute-plan.md
@/home/drewpullen/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-vector-infrastructure/02-01-SUMMARY.md

Key references:
@rlm-service/main.py — Current /query endpoint with timestamp-based chunk retrieval
@rlm-service/processors/embedding_generator.py — Has embed_text(text, dimensions=768) function
@supabase/migrations/20260211_hnsw_index_768.sql — RPC functions for similarity search
</context>

<tasks>

<task type="auto">
  <name>Task 1: Replace timestamp chunk retrieval with semantic vector search in RLM /query</name>
  <files>rlm-service/main.py</files>
  <action>
Replace the `get_conversation_chunks()` function and its usage in the `/query` endpoint with semantic vector search.

**Step 1: Add a new `search_chunks_semantic()` async function** near the top of main.py (after the existing `get_conversation_chunks` function):

```python
async def search_chunks_semantic(user_id: str, query: str, match_count: int = 8, threshold: float = 0.3) -> List[dict]:
    """Search conversation chunks by semantic similarity using Titan Embed v2 embeddings.

    Uses the embed_text() function from embedding_generator to create a query embedding,
    then calls the match_conversation_chunks Supabase RPC function for cosine similarity search.

    Falls back to get_conversation_chunks() (timestamp sort) if embedding or RPC fails.
    """
    try:
        from processors.embedding_generator import embed_text

        # Generate query embedding (768-dim Titan Embed v2)
        query_embedding = embed_text(query)

        # Call Supabase RPC for vector similarity search
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{SUPABASE_URL}/rest/v1/rpc/match_conversation_chunks",
                json={
                    "query_embedding": query_embedding,
                    "match_user_id": user_id,
                    "match_count": match_count,
                    "match_threshold": threshold,
                },
                headers={
                    "apikey": SUPABASE_SERVICE_KEY,
                    "Authorization": f"Bearer {SUPABASE_SERVICE_KEY}",
                    "Content-Type": "application/json",
                },
                timeout=15.0,
            )

            if response.status_code != 200:
                print(f"[SemanticSearch] RPC error {response.status_code}: {response.text[:200]}")
                # Fall back to timestamp sort
                return await get_conversation_chunks(user_id, recent_only=True)

            chunks = response.json()
            print(f"[SemanticSearch] Found {len(chunks)} relevant chunks for user {user_id}")
            return chunks

    except Exception as e:
        print(f"[SemanticSearch] Failed, falling back to timestamp sort: {e}")
        return await get_conversation_chunks(user_id, recent_only=True)
```

**Step 2: Update the `/query` endpoint** to call `search_chunks_semantic()` instead of `get_conversation_chunks()`.

In the `query()` function (around line 664), replace:
```python
chunks = await get_conversation_chunks(request.user_id, recent_only=True)
```
with:
```python
chunks = await search_chunks_semantic(request.user_id, request.message, match_count=8, threshold=0.3)
```

**Step 3: Update the context formatting** in the `/query` endpoint to handle the new response shape. The RPC returns objects with `title`, `content`, `similarity`, `chunk_tier` etc. Update the context building block (around line 669-671) to:

```python
# Build context from semantically-matched chunks
conversation_context = ""
for chunk in chunks[:10]:  # Top 10 most relevant
    title = chunk.get('title', 'Untitled')
    similarity = chunk.get('similarity', 0)
    content = chunk.get('content', '')[:2000]  # Truncate long chunks
    conversation_context += f"\n---\n**{title}** (relevance: {similarity:.2f})\n{content}"
```

**Step 4: Update the QueryResponse** to log semantic search info. The `method` field in the response should reflect whether semantic search was used. Update the `method` assignment:
- If `search_chunks_semantic` succeeds (chunks have 'similarity' key), log `"rlm+semantic"` or `"fallback+semantic"`
- Keep existing rlm vs fallback distinction

Do NOT remove `get_conversation_chunks()` — it's still the fallback.

**Important:** The `embed_text()` function is synchronous (uses boto3 sync client). It can be called directly in an async context — it will block briefly (~50ms) which is acceptable for a single query embedding.
  </action>
  <verify>
1. `python -c "import ast; ast.parse(open('rlm-service/main.py').read()); print('Syntax OK')"` passes
2. Grep confirms `search_chunks_semantic` function exists in main.py
3. Grep confirms `embed_text` is imported from `processors.embedding_generator`
4. Grep confirms `match_conversation_chunks` RPC call exists
5. Grep confirms `get_conversation_chunks` still exists (as fallback)
6. `npm run build` succeeds (Next.js build unaffected since only Python changed)
  </verify>
  <done>
The RLM /query endpoint embeds the user's message with Titan Embed v2, calls match_conversation_chunks RPC for cosine similarity search, and passes the top 8 semantically-relevant chunks as context. Falls back to timestamp sort if semantic search fails.
  </done>
</task>

<task type="auto">
  <name>Task 2: Verify memory_md flows through system prompt correctly</name>
  <files>rlm-service/main.py</files>
  <action>
Verify and document that memory_md already flows correctly through the prompt pipeline. The chat route sends `sections.memory = userProfile.memory_md` to RLM. The PromptBuilder already includes `## MEMORY\n{memory_section}` when memory_md is present.

**Verification steps (read-only, confirm existing behavior):**

1. In `app/api/chat/route.ts` line ~430: Confirm `sections.memory` is set from `userProfile?.memory_md`
2. In `rlm-service/main.py` `query_with_rlm()`: Confirm `sections` dict is passed to `PromptBuilder.build_emotionally_intelligent_prompt()` via `_sections_to_profile(sections, soulprint_text)` which maps `sections.get("memory")` to `memory_md`
3. In `rlm-service/prompt_builder.py` all prompt versions: Confirm `memory_section = profile.get("memory_md")` and `prompt += f"\n\n## MEMORY\n{memory_section}"` when present

**If all three checks pass (they should based on code reading), add a debug log** to the `/query` endpoint to confirm memory_md is being used:

After the `ai_name` resolution (around line 674), add:
```python
# Log memory availability for debugging
has_memory_md = bool(request.sections and request.sections.get("memory"))
print(f"[Query] user={request.user_id}, has_memory_md={has_memory_md}, chunks={len(chunks)}")
```

This log line makes MEM-01 verifiable in production RLM logs without any functional change.
  </action>
  <verify>
1. Grep confirms `has_memory_md` log line exists in main.py
2. `python -c "import ast; ast.parse(open('rlm-service/main.py').read()); print('Syntax OK')"` passes
  </verify>
  <done>
memory_md flow confirmed end-to-end: chat route sends it as sections.memory, RLM maps it to profile.memory_md, PromptBuilder includes it as ## MEMORY section. Debug log added for production observability.
  </done>
</task>

</tasks>

<verification>
1. RLM /query endpoint uses `search_chunks_semantic()` with embed_text + match_conversation_chunks RPC (not timestamp sort)
2. Fallback to `get_conversation_chunks()` still works if semantic search fails
3. memory_md included in system prompt (visible via has_memory_md log)
4. Python syntax valid for main.py
5. Next.js build still passes
</verification>

<success_criteria>
- The RLM /query endpoint generates a Titan Embed v2 query embedding and calls match_conversation_chunks RPC
- Conversation context passed to Claude contains the most semantically-relevant chunks, not just the most recent
- memory_md availability is logged for every /query call
- Fallback gracefully degrades to timestamp sort on embedding/RPC failure
</success_criteria>

<output>
After completion, create `.planning/phases/03-memory-in-chat/03-01-SUMMARY.md`
</output>
