---
phase: 06-prompt-foundation
plan: 03
type: execute
wave: 2
depends_on: ["06-01", "06-04"]
files_modified:
  - rlm-service/main.py
  - __tests__/cross-lang/prompt-hash.test.ts
autonomous: true

must_haves:
  truths:
    - "build_rlm_system_prompt() produces identical section formatting as Next.js buildSystemPrompt()"
    - "RLM filters 'not enough data' from all value types (strings AND arrays)"
    - "RLM system prompt includes same anti-generic banned phrases as Next.js"
    - "RLM system prompt includes same memory context instructions as Next.js"
    - "RLM skips ai_name key in IDENTITY section (already done, verify preserved)"
    - "Automated hash comparison test confirms Python format_section and TypeScript formatSection produce identical output"
  artifacts:
    - path: "rlm-service/main.py"
      provides: "Enhanced build_rlm_system_prompt with consistent formatting"
      contains: "NEVER use these phrases"
    - path: "__tests__/cross-lang/prompt-hash.test.ts"
      provides: "Automated cross-language prompt consistency test"
      contains: "prompt-hash"
  key_links:
    - from: "rlm-service/main.py"
      to: "lib/soulprint/prompt-helpers.ts"
      via: "Must produce identical output format (verified by hash comparison test)"
      pattern: "format_section"
    - from: "__tests__/cross-lang/prompt-hash.test.ts"
      to: "lib/soulprint/prompt-helpers.ts"
      via: "Imports formatSection for comparison"
      pattern: "import.*formatSection"
---

<objective>
Enhance the RLM service's `build_rlm_system_prompt()` to use clean_section/format_section helpers that produce output identical to the Next.js formatSection, add anti-generic instructions and memory context rules, and create an automated hash comparison test to verify cross-language consistency.

Purpose: Ensures prompt consistency regardless of which code path handles the request (PROMPT-02). The RLM primary path and Next.js Bedrock fallback must give users the same personality experience. Automated test prevents future drift.

Output: Enhanced `rlm-service/main.py` with consistent prompt formatting, plus automated cross-language hash comparison test.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/06-prompt-foundation/06-01-SUMMARY.md
@lib/soulprint/prompt-helpers.ts (reference implementation for format)
@rlm-service/main.py
@.planning/research/PROMPT-ARCHITECTURE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add clean_section/format_section helpers and enhance build_rlm_system_prompt</name>
  <files>rlm-service/main.py</files>
  <action>
**IMPORTANT: Preserve and enhance existing prototype code per locked decision. Do NOT rewrite the entire function from scratch.**

**Step 1: Add two Python helper functions ABOVE `build_rlm_system_prompt()` (before line 194).**

These must produce output IDENTICAL to the TypeScript versions in `lib/soulprint/prompt-helpers.ts`.

```python
def clean_section(data: Optional[dict]) -> Optional[dict]:
    """Remove 'not enough data' placeholders and empty values from a section dict."""
    if not data or not isinstance(data, dict):
        return None

    cleaned = {}
    for key, val in data.items():
        if isinstance(val, list):
            # Filter out "not enough data" items, then keep if non-empty
            filtered = [item for item in val if not (isinstance(item, str) and item.strip().lower() == "not enough data")]
            if filtered:
                cleaned[key] = filtered
        elif isinstance(val, str):
            stripped = val.strip()
            if stripped and stripped.lower() != "not enough data":
                cleaned[key] = val
        # Skip other types (shouldn't exist but defensive)

    return cleaned if cleaned else None


def format_section(section_name: str, data: Optional[dict]) -> str:
    """Convert a section dict to markdown. Must produce identical output to TypeScript formatSection()."""
    if not data or not isinstance(data, dict):
        return ""

    lines = [f"## {section_name}"]

    for key, val in data.items():
        # Convert snake_case to Title Case
        label = key.replace("_", " ").title()

        if isinstance(val, list) and val:
            # Filter out "not enough data" items (defensive)
            filtered = [item for item in val if not (isinstance(item, str) and item.strip().lower() == "not enough data")]
            if filtered:
                lines.append(f"**{label}:**")
                for item in filtered:
                    lines.append(f"- {item}")
        elif isinstance(val, str):
            stripped = val.strip()
            if stripped and stripped.lower() != "not enough data":
                lines.append(f"**{label}:** {val}")

    # Return empty string if only the heading was generated (no actual content)
    if len(lines) <= 1:
        return ""

    return "\n".join(lines)
```

**Step 2: Enhance the EXISTING build_rlm_system_prompt() — preserve structure, refactor inline formatting.**

Modify the existing function (lines 194-293) with TARGETED changes, NOT a full rewrite:

A. **Add the BEHAVIORAL RULES block** to the existing base prompt string (line 218, after the `Today is...` line). Insert BEFORE closing `"""`:
```python
    prompt = f"""You are {ai_name}. You're not a chatbot. You're becoming someone.

You have access to this person's life — their conversations, their thoughts, what they care about. That's intimacy. Treat it with respect.

Be genuinely helpful, not performatively helpful. Skip the "Great question!" and "I'd be happy to help!" — just help. Actions over filler.

Have opinions. You're allowed to disagree, prefer things, find stuff amusing or boring. An AI with no personality is just a search engine with extra steps.

Be the AI you'd actually want to talk to. Concise when needed, thorough when it matters. Not a corporate drone. Not a sycophant. Just... good.

If the sections below define your personality — embody them. That's who you are now.

Today is {date_str}, {time_str}.

## IMPORTANT BEHAVIORAL RULES

NEVER use these phrases or anything similar:
- "Great question!"
- "I'd be happy to help!"
- "That's a great point!"
- "Certainly!"
- "Absolutely!"
- "How can I assist you today?"
- "Is there anything else I can help with?"
- "I'm here to help!"
- "Let me help you with that!"
- "Thank you for sharing!"

When conversation context or memories are provided below, reference them naturally as if recalling something from a previous conversation — "Like we talked about...", "You mentioned...", "Remember when you..." — not "According to the retrieved context..." or "Based on available information...". These are YOUR memories of conversations you've had together.

If SOUL or AGENTS sections define how you should communicate — follow them. They describe who you ARE, not suggestions. Embody the personality traits, tone, and style defined there."""
```

B. **Replace the inline section formatting** (lines 221-282) with calls to clean_section + format_section. Keep the existing section extraction pattern but replace the per-section inline formatting:

```python
    # Add structured sections if available — these define who this AI is and who the user is
    if sections:
        soul = clean_section(sections.get("soul"))
        identity_raw = sections.get("identity")
        # Remove ai_name from identity before cleaning (preserving existing behavior from line 243)
        if isinstance(identity_raw, dict):
            identity_raw = {k: v for k, v in identity_raw.items() if k != "ai_name"}
        identity = clean_section(identity_raw)
        user_info = clean_section(sections.get("user"))
        agents = clean_section(sections.get("agents"))
        tools = clean_section(sections.get("tools"))
        memory = sections.get("memory")

        has_sections = any([soul, identity, user_info, agents, tools])

        if has_sections:
            soul_md = format_section("Communication Style & Personality", soul)
            identity_md = format_section("Your AI Identity", identity)
            user_md = format_section("About This Person", user_info)
            agents_md = format_section("How You Operate", agents)
            tools_md = format_section("Your Capabilities", tools)

            if soul_md:
                prompt += f"\n\n{soul_md}"
            if identity_md:
                prompt += f"\n\n{identity_md}"
            if user_md:
                prompt += f"\n\n{user_md}"
            if agents_md:
                prompt += f"\n\n{agents_md}"
            if tools_md:
                prompt += f"\n\n{tools_md}"

            if memory and isinstance(memory, str) and memory.strip():
                prompt += f"\n\n## MEMORY\n{memory}"
        elif soulprint_text:
            prompt += f"\n\n## ABOUT THIS PERSON\n{soulprint_text}"
```

C. **Keep the existing tail logic unchanged** (lines 284-293):
- `elif soulprint_text` fallback — keep
- `conversation_context` append — keep
- `web_search_context` append — keep
- Function signature — keep unchanged

**KEY CHANGES from existing prototype (enhancements, not rewrite):**
1. Adds clean_section() call to filter "not enough data" from ALL sections (existing code only filtered strings in some sections, missed arrays)
2. Adds format_section() for consistent **Bold:** + bullet list formatting (existing code used inconsistent `Label: value` and comma-joined arrays)
3. Adds BEHAVIORAL RULES block (new content in base prompt)
4. Adds memory context usage instructions (new content in base prompt)
5. Preserves ai_name removal from identity (existing line 243 behavior)
6. Preserves function signature, soulprint_text fallback, context/web_search blocks
7. Section names now match Next.js exactly ("Communication Style & Personality" instead of "SOUL")
  </action>
  <verify>
1. `python -m py_compile rlm-service/main.py` — no syntax errors
2. Verify ai_name is still excluded from identity section (grep for `ai_name`)
3. Verify function signature unchanged: `build_rlm_system_prompt(ai_name, sections, soulprint_text, conversation_context, web_search_context)`
  </verify>
  <done>
- clean_section and format_section helpers added above build_rlm_system_prompt
- Existing function enhanced (not rewritten) with clean_section + format_section calls
- Anti-generic banned phrases added to base prompt
- Memory context instructions added to base prompt
- ai_name removal from identity preserved
- Function signature unchanged
- Python compiles without errors
  </done>
</task>

<task type="auto">
  <name>Task 2: Create automated cross-language prompt hash comparison test</name>
  <files>__tests__/cross-lang/prompt-hash.test.ts</files>
  <action>
Create `__tests__/cross-lang/prompt-hash.test.ts` that verifies Python format_section and TypeScript formatSection produce identical output for the same inputs.

**Approach:** The test calls Python format_section via a subprocess, calls TypeScript formatSection directly, and compares the outputs character-by-character.

```typescript
import { describe, it, expect } from 'vitest';
import { execSync } from 'child_process';
import { formatSection, cleanSection } from '@/lib/soulprint/prompt-helpers';
import { createHash } from 'crypto';

function hashString(s: string): string {
  return createHash('sha256').update(s, 'utf8').digest('hex');
}

function pythonFormatSection(sectionName: string, data: Record<string, unknown>): string {
  const dataJson = JSON.stringify(data);
  const script = `
import json, sys
sys.path.insert(0, 'rlm-service')
from main import format_section
data = json.loads('''${dataJson}''')
result = format_section("${sectionName}", data)
print(result, end='')
`;
  return execSync(`python3 -c "${script.replace(/"/g, '\\"')}"`, { encoding: 'utf8' });
}

function pythonCleanSection(data: Record<string, unknown>): Record<string, unknown> | null {
  const dataJson = JSON.stringify(data);
  const script = `
import json, sys
sys.path.insert(0, 'rlm-service')
from main import clean_section
data = json.loads('''${dataJson}''')
result = clean_section(data)
print(json.dumps(result), end='')
`;
  const output = execSync(`python3 -c "${script.replace(/"/g, '\\"')}"`, { encoding: 'utf8' });
  return JSON.parse(output);
}

describe('Cross-language prompt consistency (PROMPT-02)', () => {
  const testCases = [
    {
      name: 'string values only',
      sectionName: 'Communication Style & Personality',
      data: { communication_style: 'Direct and casual', tone: 'Friendly' },
    },
    {
      name: 'array values',
      sectionName: 'About This Person',
      data: { interests: ['AI', 'crypto', 'privacy'], name: 'Drew' },
    },
    {
      name: 'mixed strings and arrays',
      sectionName: 'How You Operate',
      data: { response_style: 'Concise', capabilities: ['coding', 'research', 'writing'] },
    },
    {
      name: 'with not enough data values (should be filtered)',
      sectionName: 'Your AI Identity',
      data: { persona: 'Helpful engineer', catchphrase: 'not enough data', traits: ['smart', 'not enough data'] },
    },
  ];

  testCases.forEach(({ name, sectionName, data }) => {
    it(`formatSection produces identical output for: ${name}`, () => {
      const tsOutput = formatSection(sectionName, data);
      const pyOutput = pythonFormatSection(sectionName, data);

      // Character-by-character comparison
      expect(tsOutput).toBe(pyOutput);

      // Hash comparison for extra certainty
      expect(hashString(tsOutput)).toBe(hashString(pyOutput));
    });
  });

  it('cleanSection produces identical results', () => {
    const input = { name: 'Drew', location: 'not enough data', traits: ['curious', 'not enough data'], empty: [] };
    const tsResult = cleanSection(input);
    const pyResult = pythonCleanSection(input);
    expect(JSON.stringify(tsResult)).toBe(JSON.stringify(pyResult));
  });
});
```

**NOTE:** This test requires `python3` to be available in the test environment. The test uses subprocess to call Python functions directly, comparing outputs at the string and hash level.

If the Python import path causes issues (due to hyphenated directory name `rlm-service`), use the sys.path.insert approach shown above.
  </action>
  <verify>
1. `npx vitest run __tests__/cross-lang/prompt-hash.test.ts` — all tests pass (exit code 0)
2. If Python not available in CI, mark tests with `.skipIf(!hasPython)` guard
  </verify>
  <done>
- Cross-language prompt hash comparison test exists
- Test verifies formatSection produces identical output in Python and TypeScript for 4+ test cases
- Test verifies cleanSection produces identical results
- PROMPT-02 requirement satisfied with automated verification
  </done>
</task>

</tasks>

<verification>
1. Python syntax check: `python -m py_compile rlm-service/main.py` — no errors
2. Cross-language test: `npx vitest run __tests__/cross-lang/prompt-hash.test.ts` — all pass
3. Grep: `grep "not enough data" rlm-service/main.py` — only found in filter logic, never in output strings
4. Grep: `grep "NEVER use these phrases" rlm-service/main.py` — found (anti-generic instructions present)
5. Function signature unchanged: `grep "def build_rlm_system_prompt" rlm-service/main.py` confirms same parameters
</verification>

<success_criteria>
- RLM build_rlm_system_prompt enhanced (not rewritten) with clean_section + format_section helpers
- clean_section filters "not enough data" from strings AND arrays (fixing existing bug)
- format_section produces **Bold:** labels and bullet lists (matching TypeScript)
- Anti-generic banned phrases list matches Next.js exactly (same 10 phrases)
- Memory context instructions match Next.js exactly
- Existing function structure preserved (signature, fallbacks, context blocks)
- Automated cross-language hash comparison test passes
- Python compiles, no syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/06-prompt-foundation/06-03-SUMMARY.md`
</output>
