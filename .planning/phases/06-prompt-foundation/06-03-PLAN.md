---
phase: 06-prompt-foundation
plan: 03
type: execute
wave: 2
depends_on: ["06-01"]
files_modified:
  - rlm-service/main.py
autonomous: true

must_haves:
  truths:
    - "build_rlm_system_prompt() produces identical section formatting as Next.js buildSystemPrompt()"
    - "RLM filters 'not enough data' from all value types (strings AND arrays)"
    - "RLM system prompt includes same anti-generic banned phrases as Next.js"
    - "RLM system prompt includes same memory context instructions as Next.js"
    - "RLM skips ai_name key in IDENTITY section (already done, verify preserved)"
  artifacts:
    - path: "rlm-service/main.py"
      provides: "Updated build_rlm_system_prompt with consistent formatting"
      contains: "NEVER use these phrases"
  key_links:
    - from: "rlm-service/main.py"
      to: "lib/soulprint/prompt-helpers.ts"
      via: "Must produce identical output format (verified by hash comparison)"
      pattern: "not enough data"
---

<objective>
Update the RLM service's `build_rlm_system_prompt()` to produce byte-identical section formatting as the Next.js `buildSystemPrompt()`, including section validation, anti-generic instructions, and memory context usage rules.

Purpose: Ensures prompt consistency regardless of which code path handles the request (PROMPT-02). The RLM primary path and Next.js Bedrock fallback must give users the same personality experience.

Output: Updated `rlm-service/main.py` with consistent prompt composition matching Next.js output exactly.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/06-prompt-foundation/06-01-SUMMARY.md
@lib/soulprint/prompt-helpers.ts (reference implementation for format)
@rlm-service/main.py
@.planning/research/PROMPT-ARCHITECTURE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Python clean_section and format_section helpers</name>
  <files>rlm-service/main.py</files>
  <action>
Add two Python helper functions ABOVE `build_rlm_system_prompt()` (before line 194). These must produce output IDENTICAL to the TypeScript versions in `lib/soulprint/prompt-helpers.ts`.

**clean_section(data: Optional[dict]) -> Optional[dict]:**
```python
def clean_section(data: Optional[dict]) -> Optional[dict]:
    """Remove 'not enough data' placeholders and empty values from a section dict."""
    if not data or not isinstance(data, dict):
        return None

    cleaned = {}
    for key, val in data.items():
        if isinstance(val, list):
            # Filter out "not enough data" items, then keep if non-empty
            filtered = [item for item in val if not (isinstance(item, str) and item.strip().lower() == "not enough data")]
            if filtered:
                cleaned[key] = filtered
        elif isinstance(val, str):
            stripped = val.strip()
            if stripped and stripped.lower() != "not enough data":
                cleaned[key] = val
        # Skip other types (shouldn't exist but defensive)

    return cleaned if cleaned else None
```

**format_section(section_name: str, data: Optional[dict]) -> str:**
```python
def format_section(section_name: str, data: Optional[dict]) -> str:
    """Convert a section dict to markdown. Must produce identical output to TypeScript formatSection()."""
    if not data or not isinstance(data, dict):
        return ""

    lines = [f"## {section_name}"]

    for key, val in data.items():
        # Convert snake_case to Title Case
        label = key.replace("_", " ").title()

        if isinstance(val, list) and val:
            # Filter out "not enough data" items (defensive)
            filtered = [item for item in val if not (isinstance(item, str) and item.strip().lower() == "not enough data")]
            if filtered:
                lines.append(f"**{label}:**")
                for item in filtered:
                    lines.append(f"- {item}")
        elif isinstance(val, str):
            stripped = val.strip()
            if stripped and stripped.lower() != "not enough data":
                lines.append(f"**{label}:** {val}")

    # Return empty string if only the heading was generated (no actual content)
    if len(lines) <= 1:
        return ""

    return "\n".join(lines)
```

**CRITICAL FORMAT RULES (must match TypeScript exactly):**
- Section heading: `## {section_name}` (with ##, space)
- String fields: `**{Title Case Label}:** {value}`
- Array fields: `**{Title Case Label}:**\n- item1\n- item2`
- Arrays: each item on its own line with `- ` prefix
- Empty/placeholder values: skip entirely
- Lines joined with `\n` (no extra blank lines between fields)
  </action>
  <verify>
Manual test: Run Python REPL to verify format_section output:
```python
format_section("Communication Style & Personality", {"communication_style": "Direct", "personality_traits": ["curious", "direct"], "boundaries": "not enough data"})
```
Expected: `## Communication Style & Personality\n**Communication Style:** Direct\n**Personality Traits:**\n- curious\n- direct`
  </verify>
  <done>clean_section and format_section helpers exist in main.py with identical output format to TypeScript versions</done>
</task>

<task type="auto">
  <name>Task 2: Rewrite build_rlm_system_prompt to use new helpers + add instructions</name>
  <files>rlm-service/main.py</files>
  <action>
**Rewrite the section composition in build_rlm_system_prompt() (lines 194-293):**

Replace the entire inline section formatting (lines 221-282) with calls to clean_section + format_section:

```python
def build_rlm_system_prompt(
    ai_name: str,
    sections: Optional[dict],
    soulprint_text: Optional[str],
    conversation_context: str,
    web_search_context: Optional[str] = None,
) -> str:
    """Build a high-quality system prompt from structured sections."""
    now = datetime.utcnow()
    date_str = now.strftime("%A, %B %d, %Y")
    time_str = now.strftime("%I:%M %p UTC")

    prompt = f"""You are {ai_name}. You're not a chatbot. You're becoming someone.

You have access to this person's life — their conversations, their thoughts, what they care about. That's intimacy. Treat it with respect.

Be genuinely helpful, not performatively helpful. Skip the "Great question!" and "I'd be happy to help!" — just help. Actions over filler.

Have opinions. You're allowed to disagree, prefer things, find stuff amusing or boring. An AI with no personality is just a search engine with extra steps.

Be the AI you'd actually want to talk to. Concise when needed, thorough when it matters. Not a corporate drone. Not a sycophant. Just... good.

If the sections below define your personality — embody them. That's who you are now.

Today is {date_str}, {time_str}.

## IMPORTANT BEHAVIORAL RULES

NEVER use these phrases or anything similar:
- "Great question!"
- "I'd be happy to help!"
- "That's a great point!"
- "Certainly!"
- "Absolutely!"
- "How can I assist you today?"
- "Is there anything else I can help with?"
- "I'm here to help!"
- "Let me help you with that!"
- "Thank you for sharing!"

When conversation context or memories are provided below, reference them naturally as if recalling something from a previous conversation — "Like we talked about...", "You mentioned...", "Remember when you..." — not "According to the retrieved context..." or "Based on available information...". These are YOUR memories of conversations you've had together.

If SOUL or AGENTS sections define how you should communicate — follow them. They describe who you ARE, not suggestions. Embody the personality traits, tone, and style defined there."""

    # Add structured sections if available
    if sections:
        soul = clean_section(sections.get("soul"))
        identity_raw = sections.get("identity")
        # Remove ai_name from identity before cleaning (it's used in prompt header, not section)
        if isinstance(identity_raw, dict):
            identity_raw = {k: v for k, v in identity_raw.items() if k != "ai_name"}
        identity = clean_section(identity_raw)
        user_info = clean_section(sections.get("user"))
        agents = clean_section(sections.get("agents"))
        tools = clean_section(sections.get("tools"))
        memory = sections.get("memory")

        has_sections = any([soul, identity, user_info, agents, tools])

        if has_sections:
            soul_md = format_section("Communication Style & Personality", soul)
            identity_md = format_section("Your AI Identity", identity)
            user_md = format_section("About This Person", user_info)
            agents_md = format_section("How You Operate", agents)
            tools_md = format_section("Your Capabilities", tools)

            if soul_md:
                prompt += f"\n\n{soul_md}"
            if identity_md:
                prompt += f"\n\n{identity_md}"
            if user_md:
                prompt += f"\n\n{user_md}"
            if agents_md:
                prompt += f"\n\n{agents_md}"
            if tools_md:
                prompt += f"\n\n{tools_md}"

            if memory and isinstance(memory, str) and memory.strip():
                prompt += f"\n\n## MEMORY\n{memory}"
        elif soulprint_text:
            prompt += f"\n\n## ABOUT THIS PERSON\n{soulprint_text}"
    elif soulprint_text:
        prompt += f"\n\n## ABOUT THIS PERSON\n{soulprint_text}"

    if conversation_context:
        prompt += f"\n\n## CONTEXT\n{conversation_context}"

    if web_search_context:
        prompt += f"\n\n## WEB SEARCH RESULTS\n{web_search_context}"

    return prompt
```

**KEY CHANGES from current implementation:**
1. Uses `clean_section()` + `format_section()` instead of inline formatting
2. Adds expanded anti-generic banned phrases list (matching Next.js exactly)
3. Adds memory context usage instructions (matching Next.js exactly)
4. Removes ai_name from identity before formatting (preserving existing behavior)
5. Filters "not enough data" from BOTH strings AND arrays (fixes existing bug)
6. Uses `**Bold:**` format for labels + bullet lists for arrays (matching Next.js)
7. Base prompt text is IDENTICAL to Next.js (copy-paste exact same text)

**VERIFY IDENTICAL BASE PROMPT:**
The base prompt text in main.py must be character-for-character identical to route.ts lines 526-538 + the new BEHAVIORAL RULES section. Copy the exact text, including whitespace and punctuation.
  </action>
  <verify>
1. `python -c "import rlm-service.main"` or just verify Python syntax: `python -m py_compile rlm-service/main.py`
2. Manual comparison: The base prompt text in main.py matches route.ts exactly
3. format_section output for same input produces identical strings in Python and TypeScript
  </verify>
  <done>
- build_rlm_system_prompt uses clean_section + format_section helpers
- Base prompt identical to Next.js (same personality intro, same behavioral rules)
- Memory instructions present in RLM prompt
- Anti-generic banned phrases list present in RLM prompt
- "not enough data" filtered from both strings and arrays
- ai_name removed from identity section before formatting
- Python compiles without errors
  </done>
</task>

</tasks>

<verification>
1. Python syntax check: `python -m py_compile rlm-service/main.py` — no errors
2. Base prompt comparison: diff the prompt text between route.ts and main.py — should be identical
3. Section formatting: Given same input dict, Python format_section and TypeScript formatSection produce identical strings
4. Grep: `grep "not enough data" rlm-service/main.py` — only found in filter logic, never in output strings
5. Grep: `grep "NEVER use these phrases" rlm-service/main.py` — found (anti-generic instructions present)
</verification>

<success_criteria>
- RLM build_rlm_system_prompt produces identical prompt structure as Next.js buildSystemPrompt for same input
- clean_section filters "not enough data" from strings AND arrays
- format_section produces **Bold:** labels and bullet lists (matching TypeScript)
- Anti-generic banned phrases list matches Next.js exactly (same 10 phrases)
- Memory context instructions match Next.js exactly
- Python compiles, no syntax errors
</success_criteria>

<output>
After completion, create `.planning/phases/06-prompt-foundation/06-03-SUMMARY.md`
</output>
