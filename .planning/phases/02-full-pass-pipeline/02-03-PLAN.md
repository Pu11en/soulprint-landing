---
phase: 02-full-pass-pipeline
plan: 03
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - rlm-service/processors/v2_regenerator.py
  - rlm-service/processors/full_pass.py
  - rlm-service/main.py
autonomous: true

must_haves:
  truths:
    - "After full pass completes, all 5 section columns (soul_md, identity_md, user_md, agents_md, tools_md) contain v2 content regenerated from complete data"
    - "V2 regeneration uses a larger conversation sample (top 200 conversations) plus the MEMORY section for context"
    - "soulprint_text is updated with concatenation of all v2 sections plus the MEMORY section"
    - "full_pass_status transitions to 'complete' only after both MEMORY generation AND v2 regeneration succeed"
  artifacts:
    - path: "rlm-service/processors/v2_regenerator.py"
      provides: "V2 section regeneration using complete data and MEMORY context"
      contains: "regenerate_sections_v2"
  key_links:
    - from: "rlm-service/processors/full_pass.py"
      to: "rlm-service/processors/v2_regenerator.py"
      via: "import regenerate_sections_v2"
      pattern: "from.*v2_regenerator import"
    - from: "rlm-service/processors/v2_regenerator.py"
      to: "supabase user_profiles"
      via: "update soul_md, identity_md, user_md, agents_md, tools_md, soulprint_text"
      pattern: "soul_md.*identity_md.*user_md"
---

<objective>
Implement v2 section regeneration that runs after MEMORY generation, using a larger conversation sample and the MEMORY section to produce richer personality sections, then save all sections and mark the full pass as complete.

Purpose: The quick pass (Phase 1) only samples ~30-50 conversations for speed. V2 regeneration uses ~200 conversations plus the MEMORY section as additional context, producing significantly richer and more nuanced SOUL, IDENTITY, USER, AGENTS, and TOOLS sections. This completes the full pass pipeline.

Output: V2 regenerator module that produces upgraded sections. Full pass pipeline orchestrates MEMORY generation followed by v2 regeneration. All database columns updated atomically. full_pass_status set to 'complete'.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-full-pass-pipeline/02-01-SUMMARY.md
@.planning/phases/02-full-pass-pipeline/02-02-SUMMARY.md

Key code to reference:
@lib/soulprint/prompts.ts (QUICK_PASS_SYSTEM_PROMPT -- reuse same prompt for v2)
@lib/soulprint/sample.ts (sampling/formatting logic -- adapt for Python)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create v2 section regenerator module</name>
  <files>rlm-service/processors/v2_regenerator.py</files>
  <action>
Create `rlm-service/processors/v2_regenerator.py` that regenerates all 5 quick-pass sections with complete data.

1. **`V2_SYSTEM_PROMPT` constant:**
   Copy the exact prompt from `lib/soulprint/prompts.ts` (QUICK_PASS_SYSTEM_PROMPT) into Python. This is the same prompt that generates the 5-section JSON object with soul, identity, user, agents, tools. The prompt is identical -- the difference is v2 gets MORE conversations as input plus the MEMORY section.

   Add one additional instruction at the end of the system prompt:
   ```
   You also have access to a MEMORY section with curated facts about this user. Use these facts to enrich your analysis -- they provide verified information that should be reflected in the sections you generate.
   ```

2. **`def sample_conversations_for_v2(conversations: list, target_count: int = 200) -> list`:**
   - Score conversations by richness (same algorithm as lib/soulprint/sample.ts):
     - message_count * 10
     - sum of user message lengths (capped at 500 each)
     - min(user_count, assistant_count) * 20
   - Sort by score descending
   - Return top `target_count` conversations (or all if fewer)
   - Filter out conversations with < 4 messages

3. **`def format_conversations_for_prompt(conversations: list, max_chars: int = 600000) -> str`:**
   - Format each conversation as:
     ```
     === Conversation: "Title" (YYYY-MM-DD) ===
     User: message content
     Assistant: response content
     ```
   - Truncate individual messages at 2000 chars
   - Stop adding conversations when total chars exceeds max_chars (600K chars ~ 150K tokens, leaving room for prompt and response within 200K context)
   - Return formatted string

4. **`async def regenerate_sections_v2(conversations: list, memory_md: str, anthropic_client) -> dict`:**
   - Sample top 200 conversations using `sample_conversations_for_v2()`
   - Format them using `format_conversations_for_prompt()`
   - Construct user message: formatted conversations + "\n\n## MEMORY (verified facts about this user)\n" + memory_md
   - Call Haiku 4.5 with:
     - system: V2_SYSTEM_PROMPT
     - user message: formatted conversations + MEMORY
     - model: `claude-haiku-4-5-20251001`
     - max_tokens: 8192
     - temperature: 0.7
   - Parse response as JSON
   - Validate that response has all 5 keys (soul, identity, user, agents, tools)
   - If parsing fails, retry once with a "Your response must be valid JSON" nudge
   - If retry fails, return None (v1 sections remain, which is acceptable)
   - Return the parsed dict with all 5 sections

5. **`def sections_to_soulprint_text(sections: dict, memory_md: str) -> str`:**
   - Converts the sections dict + MEMORY into a single markdown string
   - Same format as `sectionsToSoulprintText()` in lib/soulprint/quick-pass.ts but also includes MEMORY
   - Format:
     ```
     ## Communication Style & Personality
     **Communication Style:** ...
     **Personality Traits:**
     - trait1
     - trait2
     ...

     ## Your AI Identity
     ...

     ## About You
     ...

     ## How I Operate
     ...

     ## My Capabilities
     ...

     ## Memory
     {memory_md}
     ```
   - Each section iterates key/value pairs: arrays become bullet lists, strings become labeled paragraphs
   - Returns the full markdown string

IMPORTANT:
- The V2_SYSTEM_PROMPT must match the schema from lib/soulprint/prompts.ts EXACTLY (same JSON keys, same field descriptions). Only the instruction about MEMORY is added.
- Model ID: `claude-haiku-4-5-20251001` (Anthropic API, NOT Bedrock)
- Response must be valid JSON -- use `json.loads()` to parse, handle JSONDecodeError
- If context is too large (>200K tokens estimated), reduce the conversation sample to 100 instead of 200
  </action>
  <verify>
1. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/processors/v2_regenerator.py').read()); print('v2_regen OK')"`
2. Verify V2_SYSTEM_PROMPT contains all 5 section schemas (soul, identity, user, agents, tools)
3. Verify sample_conversations_for_v2 uses scoring algorithm matching lib/soulprint/sample.ts
4. Verify regenerate_sections_v2 includes memory_md in the user message
5. Verify sections_to_soulprint_text includes MEMORY section in output
  </verify>
  <done>
V2 regenerator samples top 200 conversations, formats them with MEMORY context, calls Haiku 4.5 with the same 5-section schema as quick pass, and produces upgraded sections. Includes soulprint_text generation that concatenates all sections plus MEMORY.
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire v2 regeneration into full pass pipeline and finalize status flow</name>
  <files>
    rlm-service/processors/full_pass.py
    rlm-service/main.py
  </files>
  <action>
**Update `rlm-service/processors/full_pass.py`:**

Modify `run_full_pass_pipeline()` to add v2 regeneration after MEMORY generation:

After the existing line that saves memory_md:
```python
# === V2 Section Regeneration ===
from processors.v2_regenerator import regenerate_sections_v2, sections_to_soulprint_text

print(f"[FullPass] Starting v2 section regeneration for user {user_id}")
v2_sections = await regenerate_sections_v2(conversations, memory_md, client)

if v2_sections:
    # Build soulprint_text from v2 sections + MEMORY
    soulprint_text = sections_to_soulprint_text(v2_sections, memory_md)

    # Save all v2 sections + soulprint_text to database atomically
    import json
    await update_user_profile(user_id, {
        "soul_md": json.dumps(v2_sections["soul"]),
        "identity_md": json.dumps(v2_sections["identity"]),
        "user_md": json.dumps(v2_sections["user"]),
        "agents_md": json.dumps(v2_sections["agents"]),
        "tools_md": json.dumps(v2_sections["tools"]),
        "soulprint_text": soulprint_text,
    })
    print(f"[FullPass] V2 sections saved for user {user_id}")
else:
    print(f"[FullPass] V2 regeneration failed -- keeping v1 sections for user {user_id}")
    # Still save soulprint_text with MEMORY added to existing sections
    # This ensures MEMORY is included even if v2 regen fails
```

Make sure `run_full_pass_pipeline()` returns both `memory_md` and `v2_sections` (or None if v2 failed).

**Update `rlm-service/main.py` run_full_pass():**

Ensure the status flow is:
1. Start: `full_pass_status = 'processing'`
2. MEMORY generated: (intermediate, no status change)
3. V2 sections regenerated: (intermediate, no status change)
4. All saved: `full_pass_status = 'complete'`, `full_pass_completed_at = now()`
5. Any error: `full_pass_status = 'failed'`, `full_pass_error = str(e)[:500]`

The `run_full_pass()` function should call `run_full_pass_pipeline()` and ONLY set 'complete' after it returns without error. Do NOT set 'complete' inside the pipeline -- let the caller handle final status.

Update the status flow:
```python
async def run_full_pass(request: ProcessFullRequest):
    try:
        await update_user_profile(request.user_id, {
            "full_pass_status": "processing",
            "full_pass_started_at": datetime.utcnow().isoformat(),
            "full_pass_error": None,
        })

        from processors.full_pass import run_full_pass_pipeline
        result = await run_full_pass_pipeline(
            user_id=request.user_id,
            storage_path=request.storage_path,
            conversation_count=request.conversation_count,
        )

        await update_user_profile(request.user_id, {
            "full_pass_status": "complete",
            "full_pass_completed_at": datetime.utcnow().isoformat(),
        })

        print(f"[FullPass] Complete for user {request.user_id}")

    except Exception as e:
        print(f"[FullPass] Failed for user {request.user_id}: {e}")
        import traceback
        traceback.print_exc()
        await update_user_profile(request.user_id, {
            "full_pass_status": "failed",
            "full_pass_error": str(e)[:500],
        })
        await alert_failure(str(e), request.user_id, "Full pass failed")
```

Remove the memory_md save from `run_full_pass_pipeline()` -- it should ONLY be saved as part of the final atomic update in full_pass.py (alongside v2 sections). This ensures consistency: either ALL updates succeed or the previous state is preserved.

Wait -- actually, `memory_md` should be saved early so the user benefits from it even if v2 regen fails. Keep the intermediate `memory_md` save. But also include it in the v2 save (idempotent update).

IMPORTANT: Store section values as `json.dumps(section_dict)` -- matching the Phase 1 convention (see decision: "Store sections as JSON.stringify'd strings in TEXT *_md columns").
  </action>
  <verify>
1. Read rlm-service/processors/full_pass.py and verify:
   - run_full_pass_pipeline calls regenerate_sections_v2 after generate_memory_section
   - v2 sections are saved with json.dumps() to match Phase 1 convention
   - soulprint_text includes both v2 sections AND MEMORY
   - memory_md is saved (even if v2 regen later fails)
2. Read rlm-service/main.py and verify:
   - run_full_pass sets 'complete' only after run_full_pass_pipeline returns
   - Error handling sets 'failed' with traceback
3. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/processors/full_pass.py').read()); ast.parse(open('rlm-service/main.py').read()); print('All OK')"`
  </verify>
  <done>
Full pass pipeline now: downloads conversations -> chunks them -> saves chunks to DB -> extracts facts -> generates MEMORY -> saves memory_md -> regenerates v2 sections with MEMORY context -> saves v2 sections + soulprint_text -> marks complete. V1 sections preserved if v2 regeneration fails. Status transitions are clean: pending -> processing -> complete/failed.
  </done>
</task>

</tasks>

<verification>
1. All Python files have valid syntax
2. V2 regeneration uses same 5-section schema as Phase 1 quick pass
3. V2 sections stored as JSON.dumps() strings (matching Phase 1 convention)
4. soulprint_text includes all 5 v2 sections plus MEMORY section
5. Full pass status flow: pending -> processing -> complete (with memory_md saved as intermediate)
6. V2 regeneration failure is non-fatal -- v1 sections preserved, memory_md still saved
7. full_pass_completed_at set only when everything succeeds
</verification>

<success_criteria>
- After full pass completes, all 5 *_md columns contain richer v2 content from 200 conversations (not 30-50)
- MEMORY section is included in soulprint_text for backwards-compatible chat
- V2 regeneration failure degrades gracefully (v1 stays, MEMORY still saved)
- full_pass_status = 'complete' indicates both MEMORY and v2 are done
- Complete end-to-end pipeline: upload -> quick pass (Phase 1) -> fire-and-forget -> full pass (Phase 2) -> v2 sections in DB
</success_criteria>

<output>
After completion, create `.planning/phases/02-full-pass-pipeline/02-03-SUMMARY.md`
</output>
