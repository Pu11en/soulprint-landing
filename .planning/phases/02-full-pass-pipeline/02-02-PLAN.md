---
phase: 02-full-pass-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - rlm-service/main.py
  - rlm-service/processors/__init__.py
  - rlm-service/processors/conversation_chunker.py
  - rlm-service/processors/fact_extractor.py
  - rlm-service/processors/memory_generator.py
  - rlm-service/processors/full_pass.py
autonomous: true

must_haves:
  truths:
    - "Full pass downloads conversations from Supabase Storage and chunks them into 1500-3000 token segments"
    - "Conversation chunks are saved to the conversation_chunks table with content, conversation_id, title, chunk_tier, and token_count"
    - "Facts are extracted from conversation chunks in parallel using Haiku 4.5 with a concurrency limit of 10"
    - "If extracted facts exceed 200K tokens, they are hierarchically reduced before MEMORY generation"
    - "A MEMORY section is generated as structured markdown with Preferences, Projects, Important Dates, Beliefs & Values, and Decisions & Context subsections"
    - "memory_md column is populated with the generated MEMORY section"
  artifacts:
    - path: "rlm-service/processors/conversation_chunker.py"
      provides: "Conversation chunking with overlap and tier assignment"
      contains: "chunk_conversations"
    - path: "rlm-service/processors/fact_extractor.py"
      provides: "Parallel fact extraction from conversation chunks via Haiku 4.5"
      contains: "extract_facts_parallel"
    - path: "rlm-service/processors/memory_generator.py"
      provides: "MEMORY section generation from consolidated facts"
      contains: "generate_memory_section"
    - path: "rlm-service/processors/full_pass.py"
      provides: "Main orchestrator that runs the complete full pass pipeline"
      contains: "run_full_pass_pipeline"
  key_links:
    - from: "rlm-service/processors/full_pass.py"
      to: "rlm-service/processors/conversation_chunker.py"
      via: "import chunk_conversations"
      pattern: "from.*conversation_chunker import"
    - from: "rlm-service/processors/full_pass.py"
      to: "rlm-service/processors/fact_extractor.py"
      via: "import extract_facts_parallel"
      pattern: "from.*fact_extractor import"
    - from: "rlm-service/processors/full_pass.py"
      to: "rlm-service/processors/memory_generator.py"
      via: "import generate_memory_section"
      pattern: "from.*memory_generator import"
    - from: "rlm-service/main.py run_full_pass()"
      to: "rlm-service/processors/full_pass.py"
      via: "import run_full_pass_pipeline"
      pattern: "from processors.full_pass import"
---

<objective>
Implement the map-reduce pipeline that processes all conversations to extract facts, generate the MEMORY section, and save conversation chunks to the database.

Purpose: This is the core of the full pass -- it transforms a raw ChatGPT export into structured, searchable knowledge (conversation chunks) and curated long-term memory (MEMORY section). This enables the AI to remember specific facts, projects, dates, and preferences from the user's entire conversation history.

Output: Working map-reduce pipeline that downloads conversations, chunks them, extracts facts in parallel, consolidates, and generates a MEMORY section. Conversation chunks saved to database. memory_md column populated.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-full-pass-pipeline/02-01-SUMMARY.md
@.planning/phases/02-full-pass-pipeline/02-RESEARCH.md

Key existing code:
@rlm-service/main.py (after 02-01 modifications)
@lib/soulprint/prompts.ts (quick pass prompt reference for consistency)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create conversation chunker and fact extractor modules</name>
  <files>
    rlm-service/processors/__init__.py
    rlm-service/processors/conversation_chunker.py
    rlm-service/processors/fact_extractor.py
  </files>
  <action>
**Create `rlm-service/processors/__init__.py`:** Empty file to make processors a package.

**Create `rlm-service/processors/conversation_chunker.py`:**

This module chunks conversations into segments for both storage (conversation_chunks table) and fact extraction.

1. **`estimate_tokens(text: str) -> int`:** Simple estimator: `len(text) // 4`. Good enough for chunking purposes.

2. **`format_conversation(conversation: dict) -> str`:**
   - Takes a raw conversation dict (from ChatGPT export format: `{"title": "...", "mapping": {...}}` or simplified format `{"title": "...", "messages": [...]}`)
   - Extracts messages in order
   - Formats as: `"User: {content}\nAssistant: {content}\n..."` (skip system messages)
   - Truncate individual messages at 5000 chars
   - Returns formatted string

3. **`chunk_conversations(conversations: list, target_tokens: int = 2000, overlap_tokens: int = 200) -> list[dict]`:**
   - For each conversation:
     - Format as text using `format_conversation()`
     - If total tokens <= target_tokens, create single chunk
     - If total tokens > target_tokens, split at sentence boundaries (split on `. `, `? `, `! `, `\n`) with overlap
   - Each chunk dict has:
     ```python
     {
         "conversation_id": str,  # conversation id or title hash
         "title": str,            # conversation title
         "content": str,          # chunk text
         "token_count": int,      # estimated tokens
         "chunk_index": int,      # 0-based position in conversation
         "total_chunks": int,     # total chunks for this conversation
         "chunk_tier": "medium",  # default tier for RAG
         "created_at": str,       # ISO timestamp from conversation
     }
     ```
   - Return list of all chunk dicts

4. **Handle both ChatGPT export formats:**
   - Old format: `{"title": "...", "mapping": {"node_id": {"message": {"author": {"role": "..."}, "content": {"parts": ["..."]}}}}}` -- traverse mapping to extract messages
   - New/simplified format: `{"title": "...", "messages": [{"role": "...", "content": "..."}]}`
   - The conversations stored in Supabase are in the simplified format (already parsed by process-server/route.ts before storage)

**Create `rlm-service/processors/fact_extractor.py`:**

1. **`FACT_EXTRACTION_PROMPT` constant:**
```python
FACT_EXTRACTION_PROMPT = """Extract ONLY factual, durable information from this conversation segment. Focus on:

1. **Preferences**: What does the user prefer? Communication style, tools, workflows, aesthetics.
2. **Projects**: What are they building or working on? Names, descriptions, tech stacks, timelines.
3. **Important Dates**: Birthdays, milestones, deadlines, start dates mentioned.
4. **Beliefs & Values**: What principles guide their decisions? What do they care about deeply?
5. **Decisions**: What choices did they make and why? Technical decisions, life decisions, tradeoffs.

Return a JSON object with these keys:
{
  "preferences": ["fact1", "fact2"],
  "projects": [{"name": "X", "description": "Y", "details": "Z"}],
  "dates": [{"event": "X", "date": "Y"}],
  "beliefs": ["belief1", "belief2"],
  "decisions": [{"decision": "X", "context": "Y"}]
}

RULES:
- Only include facts EXPLICITLY stated or strongly implied in the conversation
- Include timestamps/dates when available
- Skip generic small talk, greetings, and one-off questions with no lasting significance
- If no facts found in a category, return empty array
- Be concise -- each fact should be one clear sentence

Conversation segment:
"""
```

2. **`async def extract_facts_from_chunk(chunk_content: str, anthropic_client) -> dict`:**
   - Calls Haiku 4.5 with the fact extraction prompt + chunk content
   - Model ID: `claude-haiku-4-5-20251001` (Anthropic API, NOT Bedrock)
   - max_tokens: 2048, temperature: 0.3 (factual extraction needs low creativity)
   - Parse response as JSON. If parsing fails, return empty facts dict: `{"preferences": [], "projects": [], "dates": [], "beliefs": [], "decisions": []}`
   - Wrap in try/except -- return empty facts on any error (never fail the pipeline)

3. **`async def extract_facts_parallel(chunks: list[dict], anthropic_client, concurrency: int = 10) -> list[dict]`:**
   - Uses `asyncio.Semaphore(concurrency)` to limit parallel calls
   - Processes all chunks via `asyncio.gather(*tasks, return_exceptions=True)`
   - For each result that is an Exception, log warning and treat as empty facts
   - Returns list of fact dicts (one per chunk)

4. **`def consolidate_facts(all_facts: list[dict]) -> dict`:**
   - Merges all fact dicts into one consolidated dict
   - Deduplicates by simple string matching (exact duplicates only -- LLM dedup happens in MEMORY generation)
   - Counts total facts extracted
   - Returns: `{"preferences": [...], "projects": [...], "dates": [...], "beliefs": [...], "decisions": [...], "total_count": int}`

5. **`async def hierarchical_reduce(consolidated_facts: dict, anthropic_client, max_tokens: int = 150000) -> dict`:**
   - Estimates token count of consolidated facts JSON
   - If under max_tokens, return as-is
   - If over max_tokens, split facts into batches of ~50K tokens each
   - For each batch, call Haiku 4.5 with: "Consolidate and deduplicate these facts. Remove redundancies, merge related items, keep most recent when contradictions exist. Return the same JSON structure but more concise."
   - Recursively reduce until under max_tokens
   - Return consolidated facts dict

IMPORTANT:
- Use `anthropic.AsyncAnthropic` for async API calls (the anthropic library supports async natively)
- Model ID is `claude-haiku-4-5-20251001` (NOT the Bedrock model ID)
- All functions should have docstrings
- Log progress at each stage using print() (RLM service doesn't have structured logging yet)
  </action>
  <verify>
1. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/processors/conversation_chunker.py').read()); print('chunker OK')"`
2. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/processors/fact_extractor.py').read()); print('extractor OK')"`
3. Verify chunk_conversations returns dicts with all required keys
4. Verify FACT_EXTRACTION_PROMPT exists and contains all 5 categories
5. Verify extract_facts_parallel uses asyncio.Semaphore for concurrency control
  </verify>
  <done>
Conversation chunker splits conversations into ~2000 token segments with overlap. Fact extractor calls Haiku 4.5 in parallel (max 10 concurrent) to extract preferences, projects, dates, beliefs, and decisions. Consolidation merges and deduplicates. Hierarchical reduce handles exports that produce too many facts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create MEMORY generator and wire full pass orchestrator</name>
  <files>
    rlm-service/processors/memory_generator.py
    rlm-service/processors/full_pass.py
    rlm-service/main.py
  </files>
  <action>
**Create `rlm-service/processors/memory_generator.py`:**

1. **`MEMORY_GENERATION_PROMPT` constant:**
```python
MEMORY_GENERATION_PROMPT = """You are creating a MEMORY section for a personal AI assistant. This section captures durable facts about the user -- things that should be remembered long-term to provide personalized, context-aware responses.

From the extracted facts below, create a structured markdown document with these sections:

## Preferences
- Communication preferences, tool preferences, workflow preferences, aesthetic preferences
- How they like to work, learn, and receive information

## Projects
- Active and past projects with names, descriptions, and key details
- Include tech stacks, timelines, and status when available

## Important Dates
- Birthdays, anniversaries, milestones, deadlines
- Career events, project launches, personal milestones

## Beliefs & Values
- Core principles that guide their decisions
- What they care about deeply (privacy, quality, speed, etc.)
- Philosophical or professional stances

## Decisions & Context
- Key decisions they've made with context for why
- Tradeoffs they've considered
- Evolution of thinking (if they changed their mind on something, note both the old and new position)

RULES:
- Only include facts with actual substance -- skip vague or generic items
- Prefer specific over general ("prefers Tailwind CSS" over "likes CSS frameworks")
- Include dates/timeframes when available
- If a category has no substantive facts, include the heading with "No data yet."
- Keep each bullet point to one clear, concise sentence
- Total output should be 1000-5000 tokens (roughly 4-20KB of markdown)

Extracted facts:
"""
```

2. **`async def generate_memory_section(consolidated_facts: dict, anthropic_client) -> str`:**
   - Formats consolidated facts as readable JSON string
   - Calls Haiku 4.5 with MEMORY_GENERATION_PROMPT + facts JSON
   - Model: `claude-haiku-4-5-20251001`, max_tokens: 4096, temperature: 0.5
   - Returns the raw markdown string (NOT JSON-encoded -- this is human-readable markdown)
   - On failure, returns a minimal MEMORY section: `"# MEMORY\n\nMemory generation failed. Facts extracted but not yet organized.\n\nRaw fact count: {len(facts)}"`

**Create `rlm-service/processors/full_pass.py`:**

This is the main orchestrator that `run_full_pass()` in main.py calls.

1. **`async def run_full_pass_pipeline(user_id: str, storage_path: str, conversation_count: int = 0)`:**

   The function should:

   a. **Initialize Anthropic client:**
   ```python
   import anthropic
   client = anthropic.AsyncAnthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
   ```

   b. **Download conversations:**
   ```python
   from main import download_conversations
   conversations = await download_conversations(storage_path)
   print(f"[FullPass] Downloaded {len(conversations)} conversations for user {user_id}")
   ```

   c. **Chunk conversations:**
   ```python
   from processors.conversation_chunker import chunk_conversations
   chunks = chunk_conversations(conversations, target_tokens=2000, overlap_tokens=200)
   print(f"[FullPass] Created {len(chunks)} chunks from {len(conversations)} conversations")
   ```

   d. **Save chunks to database (without embeddings):**
   ```python
   # Save in batches of 100 to avoid request size limits
   for i in range(0, len(chunks), 100):
       batch = chunks[i:i+100]
       # First, delete existing chunks for this user (fresh start)
       if i == 0:
           await delete_user_chunks(user_id)
       await save_chunks_batch(user_id, batch)
   print(f"[FullPass] Saved {len(chunks)} chunks to database")
   ```

   e. **Extract facts in parallel:**
   ```python
   from processors.fact_extractor import extract_facts_parallel, consolidate_facts, hierarchical_reduce
   all_facts = await extract_facts_parallel(chunks, client, concurrency=10)
   consolidated = consolidate_facts(all_facts)
   print(f"[FullPass] Extracted {consolidated['total_count']} facts from {len(chunks)} chunks")

   # Reduce if too large
   reduced = await hierarchical_reduce(consolidated, client)
   ```

   f. **Generate MEMORY section:**
   ```python
   from processors.memory_generator import generate_memory_section
   memory_md = await generate_memory_section(reduced, client)
   print(f"[FullPass] Generated MEMORY section ({len(memory_md)} chars)")
   ```

   g. **Save MEMORY to database:**
   ```python
   from main import update_user_profile
   await update_user_profile(user_id, {"memory_md": memory_md})
   ```

   h. **Return memory_md** (v2 regeneration in Plan 02-03 will use this)

2. **Helper `async def delete_user_chunks(user_id: str)`:**
   - DELETE `{SUPABASE_URL}/rest/v1/conversation_chunks?user_id=eq.{user_id}`
   - Headers: service key auth
   - Best-effort: log error but don't throw

3. **Helper `async def save_chunks_batch(user_id: str, chunks: list[dict])`:**
   - POST `{SUPABASE_URL}/rest/v1/conversation_chunks` with the batch as JSON body
   - Each chunk needs `user_id` added to it
   - Set `chunk_tier = "medium"` (default)
   - Set `is_recent` based on whether `created_at` is within 6 months of now
   - Headers: service key auth, `Prefer: return=minimal`
   - Best-effort: log error but don't throw

**Update `rlm-service/main.py`:**

Replace the stub `run_full_pass()` function body with:
```python
async def run_full_pass(request: ProcessFullRequest):
    """Background task: run the complete full pass pipeline."""
    try:
        await update_user_profile(request.user_id, {
            "full_pass_status": "processing",
            "full_pass_started_at": datetime.utcnow().isoformat(),
            "full_pass_error": None,
        })

        from processors.full_pass import run_full_pass_pipeline
        memory_md = await run_full_pass_pipeline(
            user_id=request.user_id,
            storage_path=request.storage_path,
            conversation_count=request.conversation_count,
        )

        # V2 regeneration will be wired in Plan 02-03
        # For now, mark as complete after MEMORY generation
        await update_user_profile(request.user_id, {
            "full_pass_status": "complete",
            "full_pass_completed_at": datetime.utcnow().isoformat(),
        })

        print(f"[FullPass] Complete for user {request.user_id}")

    except Exception as e:
        print(f"[FullPass] Failed for user {request.user_id}: {e}")
        await update_user_profile(request.user_id, {
            "full_pass_status": "failed",
            "full_pass_error": str(e)[:500],
        })
        await alert_failure(str(e), request.user_id, "Full pass failed")
```

IMPORTANT:
- All Supabase REST calls use `SUPABASE_URL` and `SUPABASE_SERVICE_KEY` from environment (import from main or os.getenv)
- Use `httpx.AsyncClient()` for all HTTP calls
- The `conversation_chunks` table columns are: id (auto UUID), user_id, conversation_id, title, content, message_count, created_at, updated_at, is_recent, chunk_tier, embedding (nullable)
- Do NOT try to generate embeddings -- leave the embedding column NULL. Embeddings will be backfilled later.
  </action>
  <verify>
1. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/processors/full_pass.py').read()); print('full_pass OK')"`
2. Run `cd /home/drewpullen/clawd/soulprint-landing && python3 -c "import ast; ast.parse(open('rlm-service/processors/memory_generator.py').read()); print('memory_gen OK')"`
3. Verify run_full_pass_pipeline calls chunk_conversations, extract_facts_parallel, consolidate_facts, hierarchical_reduce, generate_memory_section in sequence
4. Verify save_chunks_batch adds user_id to each chunk
5. Verify main.py run_full_pass() imports and calls run_full_pass_pipeline
  </verify>
  <done>
Full pass pipeline downloads conversations, chunks them, saves chunks to DB, extracts facts in parallel via Haiku 4.5, consolidates/reduces facts, generates MEMORY section, and saves memory_md to user_profiles. Pipeline runs as background task without blocking user's chat.
  </done>
</task>

</tasks>

<verification>
1. All Python files in rlm-service/processors/ have valid syntax (ast.parse)
2. conversation_chunker.py exports chunk_conversations that returns dicts with required keys
3. fact_extractor.py uses asyncio.Semaphore for concurrency-limited parallel extraction
4. memory_generator.py generates structured markdown MEMORY section
5. full_pass.py orchestrates: download -> chunk -> save chunks -> extract facts -> consolidate -> reduce -> generate MEMORY -> save
6. main.py run_full_pass() calls run_full_pass_pipeline and updates status on success/failure
7. No embedding generation attempted (column left NULL)
</verification>

<success_criteria>
- Conversation chunker produces ~2000 token segments with overlap from any conversation list
- Fact extraction runs in parallel with max 10 concurrent Haiku 4.5 calls
- MEMORY section is generated as structured markdown with 5 subsections
- Conversation chunks saved to database with correct schema
- memory_md column populated after pipeline completes
- full_pass_status progresses: pending -> processing -> complete (or failed with error)
</success_criteria>

<output>
After completion, create `.planning/phases/02-full-pass-pipeline/02-02-SUMMARY.md`
</output>
