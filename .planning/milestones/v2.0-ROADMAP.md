# Milestone v2.0: AI Quality & Personalization

**Status:** SHIPPED 2026-02-09
**Phases:** 1-5
**Total Plans:** 14

## Overview

Make the AI sound genuinely human and deeply personalized through systematic evaluation, improved prompts, emotional intelligence, and quality awareness. The journey follows an evaluation-first approach: establish measurement infrastructure before changing any prompts, iterate with A/B testing, add emotional awareness and quality scoring, then validate everything together.

**Execution Order:** Phases 1 → 2 → (3 + 4 in parallel) → 5

## Phases

### Phase 1: Evaluation Foundation

**Goal:** Establish measurement infrastructure to baseline current system before making any changes
**Depends on:** Nothing (first phase)
**Requirements:** EVAL-01, EVAL-02, EVAL-03, EVAL-04
**Plans:** 2 plans

Plans:
- [x] 01-01-PLAN.md -- Core evaluation library (datasets + judges)
- [x] 01-02-PLAN.md -- Experiment runner + baseline + CLI scripts

**Success Criteria:**
1. Developer can create Opik evaluation datasets from anonymized chat history
2. Developer can run offline experiments comparing prompt variants with aggregate scores
3. LLM-as-judge scoring rubrics produce results with >70% human agreement
4. Baseline metrics exist for personality consistency, factuality, and tone matching of current v1 prompts
5. ~~Async Opik tracing adds <100ms P95 latency overhead~~ Deferred to Phase 5

### Phase 2: Prompt Template System

**Goal:** Enable natural voice system prompts while maintaining personality consistency and rollback capability
**Depends on:** Phase 1 (need evaluation framework to validate prompt changes)
**Requirements:** PRMT-01, PRMT-02, PRMT-03, PRMT-04
**Plans:** 3 plans

Plans:
- [x] 02-01-PLAN.md -- TypeScript PromptBuilder class + wire into chat route
- [x] 02-02-PLAN.md -- Python PromptBuilder + cross-language sync tests
- [x] 02-03-PLAN.md -- v2 variant in evaluation framework for A/B comparison

**Success Criteria:**
1. PROMPT_VERSION environment variable controls prompt style (v1-technical vs v2-natural-voice)
2. System prompts use flowing personality primer instead of technical markdown headers
3. Next.js buildSystemPrompt() and RLM build_rlm_system_prompt() produce identical output for same sections
4. Personality instructions appear after RAG memory retrieval in prompt structure (not overridden by chunks)
5. Personality adherence is maintained within 2% of baseline metrics from Phase 1

### Phase 3: Emotional Intelligence

**Goal:** Enable AI to detect user emotional state and adapt response style while acknowledging uncertainty
**Depends on:** Phase 2 (emotional intelligence scaffolding uses natural voice prompt patterns)
**Requirements:** EMOT-01, EMOT-02, EMOT-03
**Plans:** 3 plans

Plans:
- [x] 03-01-PLAN.md -- Emotional intelligence module + PromptBuilder extension
- [x] 03-02-PLAN.md -- Chat route integration (emotion detection, relationship arc, dynamic temperature)
- [x] 03-03-PLAN.md -- Python PromptBuilder sync + cross-language tests

**Success Criteria:**
1. AI detects user frustration, satisfaction, and confusion from text patterns in messages
2. AI adapts response style based on detected emotional state
3. AI explicitly acknowledges uncertainty instead of hallucinating
4. Relationship arc adjusts tone based on conversation history depth
5. Low-confidence responses use temperature 0.1-0.3 for factual grounding

### Phase 4: Quality Scoring

**Goal:** Score each soulprint section 0-100 so AI knows its own data confidence and low-quality profiles get refined
**Depends on:** Phase 1 (quality scoring uses LLM-as-judge patterns from evaluation infrastructure)
**Requirements:** QUAL-01, QUAL-02, QUAL-03
**Plans:** 3 plans

Plans:
- [x] 04-01-PLAN.md -- Quality judges + scoring orchestrator + DB migration
- [x] 04-02-PLAN.md -- PromptBuilder DATA CONFIDENCE section + chat route wiring
- [x] 04-03-PLAN.md -- Background refinement cron + import pipeline hook

**Success Criteria:**
1. Each soulprint section has quality scores 0-100 for completeness, coherence, specificity
2. Quality scores are stored in user_profiles.quality_breakdown JSONB column
3. Quality scores are surfaced in system prompts so AI adapts confidence level
4. Soulprints scoring below 60 on any metric are automatically flagged for refinement
5. Background refinement job improves flagged soulprints without user intervention

### Phase 5: Integration Validation

**Goal:** Validate all v2.0 components work together through regression testing, long-session testing, and latency benchmarks
**Depends on:** Phases 1-4 (all components must be built before integration testing)
**Requirements:** VALD-01, VALD-02, VALD-03
**Plans:** 3 plans

Plans:
- [x] 05-01-PLAN.md -- Prompt regression CLI + statistical validation + baseline comparison
- [x] 05-02-PLAN.md -- Long-session E2E tests + latency benchmark
- [x] 05-03-PLAN.md -- CI/CD regression workflow + quality correlation validation

**Success Criteria:**
1. Prompt regression test suite with 20-100 cases catches personality degradation before deploy
2. Long-session testing (10+ messages) shows no uncanny valley or personality drift
3. Async observability adds <100ms P95 latency overhead under 100 concurrent requests
4. Zero critical regressions compared to v1 baseline metrics
5. Quality scores correlate r>0.7 with user satisfaction metrics

---

## Milestone Summary

**Key Decisions:**
- Evaluation-first approach: build measurement before changing prompts
- Haiku 4.5 on Bedrock for emotion detection (fast, cheap, 150 max tokens)
- Fail-safe neutral default on any EI detection error (never crash chat)
- Three separate quality judge classes vs unified judge for specialized evaluation
- Fire-and-forget quality scoring in import pipeline (non-blocking)
- P97.5 percentile instead of P95 for latency (autocannon limitation)
- PR-triggered regression testing on prompt file changes only

**Issues Resolved:**
- Signin redirect race condition (redirecting to /chat caused bounce-back before cookies sync)

**Issues Deferred:**
- RLM service does not use EI parameters (only Bedrock fallback gets EI features)
- Wire emotional_state and relationship_arc into RLM request payload

**Technical Debt Incurred:**
- Database migration 20260209_quality_breakdown.sql needs manual execution in Supabase SQL Editor
- 7 pre-existing type errors in cross-language tests (Phase 3 test type mismatches)

---

_For current project status, see .planning/ROADMAP.md_
